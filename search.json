[{"path":"https://cathalbyrnegit.github.io/datapond/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2026 csolake authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (â€œSoftwareâ€), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED â€œâ€, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"package-structure","dir":"Articles","previous_headings":"","what":"Package Structure","title":"Code Walkthrough","text":"","code":"datapond/ â”œâ”€â”€ DESCRIPTION          # Package metadata â”œâ”€â”€ NAMESPACE            # Exports and imports â”œâ”€â”€ LICENSE              # MIT license â”œâ”€â”€ R/ â”‚   â”œâ”€â”€ db_connect.R     # Connection management â”‚   â”œâ”€â”€ db_read.R        # Read functions â”‚   â”œâ”€â”€ db_write.R       # Write functions â”‚   â”œâ”€â”€ upsert.R         # MERGE operations â”‚   â”œâ”€â”€ metadata.R       # Snapshot/catalog queries â”‚   â”œâ”€â”€ discovery.R      # List/exists functions â”‚   â”œâ”€â”€ maintenance.R    # Vacuum, rollback, diff â”‚   â”œâ”€â”€ docs.R           # Documentation & data dictionary â”‚   â”œâ”€â”€ preview.R        # Write preview functions â”‚   â”œâ”€â”€ browser.R        # Shiny browser launcher â”‚   â”œâ”€â”€ browser_ui.R     # Shiny browser UI module â”‚   â”œâ”€â”€ browser_server.R # Shiny browser server module â”‚   â””â”€â”€ zzz.R            # Package load/unload hooks â””â”€â”€ vignettes/     â”œâ”€â”€ concepts.Rmd     # Conceptual background     â””â”€â”€ code-walkthrough.Rmd  # This file"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"singleton-connection-pattern","dir":"Articles","previous_headings":"Core Design Decisions","what":"1. Singleton Connection Pattern","title":"Code Walkthrough","text":"package maintains single connection stored private environment: ? Prevents users accidentally creating multiple connections Simplifies API (need pass connection objects around) Ensures cleanup happens properly functions retrieve connection via .db_get_con() rather creating new ones.","code":".db_env <- new.env(parent = emptyenv())"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"mode-tracking-hive-vs-ducklake","dir":"Articles","previous_headings":"Core Design Decisions","what":"2. Mode Tracking (Hive vs DuckLake)","title":"Code Walkthrough","text":"connect, package records mode â€™re : Functions check give helpful errors call wrong one:","code":"assign(\"mode\", \"hive\", envir = .db_env)      # or \"ducklake\" if (curr_mode != \"hive\") {   stop(\"Connected in DuckLake mode. Use db_lake_read() instead...\") }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"input-validation","dir":"Articles","previous_headings":"Core Design Decisions","what":"3. Input Validation","title":"Code Walkthrough","text":"user inputs validated early clear error messages: catches problems like section = \"../../../etc/passwd\" cause harm.","code":".db_validate_name <- function(x, arg = \"name\") {   # Must be single non-empty string   # Only allows A-Z a-z 0-9 _ -   # Prevents SQL injection and path traversal }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"lazy-evaluation-with-duckdb","dir":"Articles","previous_headings":"Core Design Decisions","what":"4. Lazy Evaluation with DuckDB","title":"Code Walkthrough","text":"Read functions return lazy dplyr tables, collected data: ? Large datasets arenâ€™t loaded memory needed DuckDB can optimise full query (filters, joins, aggregations) Users call collect() â€™re ready bring data R","code":"dplyr::tbl(con, dplyr::sql(query))"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"rdb_connect-r---connection-management","dir":"Articles","previous_headings":"","what":"R/db_connect.R - Connection Management","title":"Code Walkthrough","text":"file handles connecting disconnecting data lake.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"private-environment-and-helpers","dir":"Articles","previous_headings":"R/db_connect.R - Connection Management","what":"Private Environment and Helpers","title":"Code Walkthrough","text":"creates isolated environment persists R session. store: con - DuckDB connection object mode - â€œhiveâ€ â€œducklakeâ€ data_path - root path parquet files catalog - DuckLake catalog name (DuckLake mode ) catalog_type - â€œduckdbâ€, â€œsqliteâ€, â€œpostgresâ€ (DuckLake mode ) metadata_path - path catalog file/connection string (DuckLake mode ) safe way get connection - checks validity, just existence. Used throughout package retrieve stored values.","code":".db_env <- new.env(parent = emptyenv()) .db_get_con <- function() {   # Returns the connection if it exists and is valid, NULL otherwise   if (exists(\"con\", envir = .db_env) && DBI::dbIsValid(.db_env$con)) {     return(.db_env$con)   }   NULL } .db_get <- function(name, default = NULL) {   # Safe getter with default value   if (exists(name, envir = .db_env)) {     get(name, envir = .db_env)   } else {     default   } }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"catalog-backend-helpers","dir":"Articles","previous_headings":"R/db_connect.R - Connection Management","what":"Catalog Backend Helpers","title":"Code Walkthrough","text":"internal functions handle different catalog backend types: builds correct DuckLake connection string based backend: - DuckDB: ducklake:metadata.ducklake - SQLite: ducklake:sqlite:catalog.sqlite - PostgreSQL: ducklake:postgres:dbname=... host=... DuckDB needs extra extensions loaded SQLite PostgreSQL backends.","code":".db_build_ducklake_dsn <- function(catalog_type, metadata_path) {   switch(catalog_type,     duckdb = paste0(\"ducklake:\", metadata_path),     sqlite = paste0(\"ducklake:sqlite:\", metadata_path),     postgres = paste0(\"ducklake:postgres:\", metadata_path),     stop(\"Unknown catalog_type\")   ) } .db_load_catalog_extensions <- function(con, catalog_type) {   # Always need ducklake   try(DBI::dbExecute(con, \"INSTALL ducklake\"), silent = TRUE)   DBI::dbExecute(con, \"LOAD ducklake\")      # Load backend-specific extension   if (catalog_type == \"sqlite\") {     try(DBI::dbExecute(con, \"INSTALL sqlite\"), silent = TRUE)     DBI::dbExecute(con, \"LOAD sqlite\")   } else if (catalog_type == \"postgres\") {     try(DBI::dbExecute(con, \"INSTALL postgres\"), silent = TRUE)     DBI::dbExecute(con, \"LOAD postgres\")   } }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_connect---hive-mode","dir":"Articles","previous_headings":"R/db_connect.R - Connection Management","what":"db_connect() - Hive Mode","title":"Code Walkthrough","text":"Key points: db = \":memory:\" means DuckDB runs -memory (fast, disk file) load_extensions allows loading things like httpfs S3 access reg.finalizer ensures cleanup even user forgets disconnect","code":"db_connect <- function(path = \"//CSO-NAS/DataLake\",                        db = \":memory:\",                        threads = NULL,                        memory_limit = NULL,                        load_extensions = NULL) {      # Return existing connection if valid   con <- .db_get_con()   if (!is.null(con)) return(con)      # Create new DuckDB connection   con <- DBI::dbConnect(duckdb::duckdb(), dbdir = db)      # Optional performance tuning   if (!is.null(threads)) {     DBI::dbExecute(con, sprintf(\"SET threads=%d\", as.integer(threads)))   }      # Store everything in .db_env   assign(\"con\", con, envir = .db_env)   assign(\"data_path\", path, envir = .db_env)   assign(\"mode\", \"hive\", envir = .db_env)      # Register cleanup for session end   reg.finalizer(.db_env, function(e) {     try(DBI::dbDisconnect(e$con, shutdown = TRUE), silent = TRUE)   }, onexit = TRUE)      con }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_lake_connect---ducklake-mode","dir":"Articles","previous_headings":"R/db_connect.R - Connection Management","what":"db_lake_connect() - DuckLake Mode","title":"Code Walkthrough","text":"Key points: Supports three catalog backends (DuckDB, SQLite, PostgreSQL) ATTACH statement connects DuckLake catalog DuckDB session Error messages include backend-specific troubleshooting hints","code":"db_lake_connect <- function(duckdb_db = \":memory:\",                             catalog = \"cso\",                             catalog_type = c(\"duckdb\", \"sqlite\", \"postgres\"),                             metadata_path = \"metadata.ducklake\",                             data_path = \"//CSO-NAS/DataLake\",                             snapshot_version = NULL,                             snapshot_time = NULL,                             ...) {      catalog_type <- match.arg(catalog_type)      # Create DuckDB connection   con <- DBI::dbConnect(duckdb::duckdb(), dbdir = duckdb_db)      # Load DuckLake and backend-specific extensions   .db_load_catalog_extensions(con, catalog_type)      # Build the backend-specific connection string   ducklake_dsn <- .db_build_ducklake_dsn(catalog_type, metadata_path)      # Build ATTACH statement with options   attach_opts <- c(glue::glue(\"DATA_PATH {.db_sql_quote(data_path)}\"))      if (!is.null(snapshot_version)) {     attach_opts <- c(attach_opts, glue::glue(\"SNAPSHOT_VERSION {as.integer(snapshot_version)}\"))   }      attach_sql <- glue::glue(     \"ATTACH {.db_sql_quote(ducklake_dsn)} AS {catalog} ({paste(attach_opts, collapse = ', ')})\"   )      # Attach with helpful error messages   tryCatch({     DBI::dbExecute(con, attach_sql)   }, error = function(e) {     DBI::dbDisconnect(con, shutdown = TRUE)     hint <- switch(catalog_type,       sqlite = \"Ensure the sqlite extension is available and the metadata file path is accessible.\",       postgres = \"Ensure PostgreSQL is running and the connection string is correct.\",       duckdb = \"Ensure the metadata file path is accessible.\"     )     stop(\"Failed to attach DuckLake catalog.\\n\", hint, \"\\n\\nOriginal error: \", e$message)   })      DBI::dbExecute(con, glue::glue(\"USE {catalog}\"))      # Store DuckLake-specific info including catalog_type   assign(\"mode\", \"ducklake\", envir = .db_env)   assign(\"catalog\", catalog, envir = .db_env)   assign(\"catalog_type\", catalog_type, envir = .db_env)   ... }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_hive_write---partitioned-parquet","dir":"Articles","previous_headings":"R/db_write.R - Writing Data","what":"db_hive_write() - Partitioned Parquet","title":"Code Walkthrough","text":"Key points: duckdb_register() makes R data.frames available SQL tables without copying replace_partitions mode deletes partition folders updated DuckDBâ€™s COPY ... PARTITION_BY creates hive-style folders automatically","code":"db_hive_write <- function(data, section, dataset,                           partition_by = NULL,                           mode = c(\"overwrite\", \"append\", \"ignore\", \"replace_partitions\"),                           compression = NULL,                           filename_pattern = \"data_{uuid}\") {      # Validate inputs   section <- .db_validate_name(section)   dataset <- .db_validate_name(dataset)      # Mode-specific validation   if (mode == \"append\" && is.null(partition_by)) {     stop(\"mode = 'append' requires partition_by...\")   }      # Register data as temporary view   temp_name <- .db_temp_name()   duckdb::duckdb_register(con, temp_name, data)   on.exit(duckdb::duckdb_unregister(con, temp_name))      # For replace_partitions, delete only affected folders   if (mode == \"replace_partitions\") {     part_vals <- unique(data[partition_by])     # Build paths like year=2024/month=01     # Delete existing folders for those partitions     fs::dir_delete(existing_partition_folders)   }      # Build COPY statement   opts <- c(\"FORMAT PARQUET\", \"PARTITION_BY (...)\")   sql <- glue::glue(\"COPY {temp_name} TO '{path}' ({opts})\")   DBI::dbExecute(con, sql) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_lake_write---ducklake-tables","dir":"Articles","previous_headings":"R/db_write.R - Writing Data","what":"db_lake_write() - DuckLake Tables","title":"Code Walkthrough","text":"Key points: Uses transactions (BEGIN/COMMIT) atomicity ROLLBACK error ensures partial changes ducklake_set_commit_message(catalog, author, message) records metadata snapshot","code":"db_lake_write <- function(data, schema = \"main\", table,                           mode = c(\"overwrite\", \"append\"),                           commit_author = NULL,                           commit_message = NULL) {      # Register data temporarily   tmp <- .db_temp_name()   duckdb::duckdb_register(con, tmp, data)      DBI::dbExecute(con, \"BEGIN\")      # Set commit metadata if provided   if (!is.null(commit_author) || !is.null(commit_message)) {     DBI::dbExecute(con, glue::glue(       \"CALL ducklake_set_commit_message({.db_sql_quote(catalog)}, {author}, {msg})\"     ))   }      # Write data   if (mode == \"overwrite\") {     sql <- glue::glue(\"CREATE OR REPLACE TABLE {qname} AS SELECT * FROM {tmp}\")   } else {     sql <- glue::glue(\"INSERT INTO {qname} SELECT * FROM {tmp}\")   }      tryCatch({     DBI::dbExecute(con, sql)     DBI::dbExecute(con, \"COMMIT\")   }, error = function(e) {     DBI::dbExecute(con, \"ROLLBACK\")     stop(e$message)   }) }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_upsert---update-insert","dir":"Articles","previous_headings":"R/upsert.R - MERGE Operations","what":"db_upsert() - Update + Insert","title":"Code Walkthrough","text":"Key points: MERGE standard SQL upsert operations strict = TRUE prevents accidental duplicates (common data quality issue) update_cols = NULL means update everything; character(0) means insert-wrapped transaction safety","code":"db_upsert <- function(data, schema = \"main\", table, by,                       strict = TRUE, update_cols = NULL,                       commit_author = NULL, commit_message = NULL) {      # Validate key columns exist in both data and target   missing_keys <- setdiff(by, names(data))   if (length(missing_keys) > 0) stop(...)      # Strict mode: reject duplicate keys in incoming data   if (strict) {     dup_check_sql <- glue::glue(\"       SELECT {by_sql}, COUNT(*) AS n       FROM {tmp}       GROUP BY {by_sql}       HAVING COUNT(*) > 1     \")     dups <- DBI::dbGetQuery(con, dup_check_sql)     if (nrow(dups) > 0) stop(\"Duplicate keys found...\")   }      # Build MERGE statement   on_sql <- paste(glue::glue(\"t.{by} = s.{by}\"), collapse = \" AND \")      # UPDATE clause depends on update_cols   update_clause <- if (is.null(update_cols)) {     \"WHEN MATCHED THEN UPDATE\"  # DuckDB shorthand: update all   } else if (length(update_cols) == 0) {     \"\"  # Insert-only mode   } else {     glue::glue(\"WHEN MATCHED THEN UPDATE SET {update_sql}\")   }      merge_sql <- glue::glue(\"     MERGE INTO {qname} AS t     USING {tmp} AS s     ON ({on_sql})     {update_clause}     WHEN NOT MATCHED THEN INSERT ...   \")      # Execute in transaction   DBI::dbExecute(con, \"BEGIN\")   DBI::dbExecute(con, merge_sql)   DBI::dbExecute(con, \"COMMIT\") }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"metadata-storage","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"Metadata Storage","title":"Code Walkthrough","text":"Documentation metadata stored differently mode: Hive mode: JSON sidecar files DuckLake mode: Tables _metadata schema","code":".db_metadata_path <- function(section, dataset) {   file.path(base_path, section, dataset, \"_metadata.json\") } .db_ensure_metadata_table <- function(con, catalog) {   DBI::dbExecute(con, \"CREATE SCHEMA IF NOT EXISTS {catalog}._metadata\")   DBI::dbExecute(con, \"CREATE TABLE IF NOT EXISTS {catalog}._metadata.table_docs (...)\")   DBI::dbExecute(con, \"CREATE TABLE IF NOT EXISTS {catalog}._metadata.column_docs (...)\") }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_describe---document-a-dataset","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"db_describe() - Document a Dataset","title":"Code Walkthrough","text":"","code":"db_describe <- function(section = NULL, dataset = NULL,                         schema = \"main\", table = NULL,                         description = NULL, owner = NULL, tags = NULL) {      if (curr_mode == \"hive\") {     # Read existing metadata, update fields, write back     meta_path <- .db_metadata_path(section, dataset)     metadata <- .db_read_metadata(meta_path)          if (!is.null(description)) metadata$description <- description     if (!is.null(owner)) metadata$owner <- owner     if (!is.null(tags)) metadata$tags <- as.character(tags)          .db_write_metadata(metadata, meta_path)        } else {     # DuckLake - upsert into _metadata.table_docs     .db_ensure_metadata_table(con, catalog)          DBI::dbExecute(con, \"DELETE FROM ... WHERE schema = ... AND table = ...\")     DBI::dbExecute(con, \"INSERT INTO ... VALUES (...)\")   } }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_dictionary---generate-data-dictionary","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"db_dictionary() - Generate Data Dictionary","title":"Code Walkthrough","text":"","code":"db_dictionary <- function(section = NULL, schema = NULL, include_columns = TRUE) {      if (curr_mode == \"hive\") {     # Iterate through sections and datasets     for (sec in db_list_sections()) {       for (ds in db_list_datasets(sec)) {         # Get metadata         meta <- db_get_docs(section = sec, dataset = ds)                  # Get columns from parquet schema         cols_info <- DBI::dbGetQuery(con, glue::glue(           \"DESCRIBE SELECT * FROM read_parquet('{path}/**/*.parquet')\"         ))                  # Combine into rows       }     }   } else {     # DuckLake - query information_schema and _metadata tables     tables <- DBI::dbGetQuery(con, \"SELECT * FROM information_schema.tables ...\")          for (tbl in tables) {       meta <- db_get_docs(schema = ..., table = ...)       cols <- DBI::dbGetQuery(con, \"SELECT * FROM information_schema.columns ...\")     }   }      do.call(rbind, rows) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_search---find-datasets","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"db_search() - Find Datasets","title":"Code Walkthrough","text":"","code":"db_search <- function(pattern, field = c(\"all\", \"name\", \"description\", \"owner\", \"tags\")) {   # Get full dictionary (without columns for speed)   dict <- db_dictionary(include_columns = FALSE)    # Filter by pattern match   matches <- switch(field,     all = grepl(pattern, dict$name) | grepl(pattern, dict$description) | ...,     name = grepl(pattern, dict$name),     description = grepl(pattern, dict$description),     ...   )    dict[matches, ] }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"public-catalog-functions","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"Public Catalog Functions","title":"Code Walkthrough","text":"public catalog enables organisation-wide data discovery without requiring access actual data.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"hive-mode-_catalog-folder","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary > Public Catalog Functions","what":"Hive Mode: _catalog/ Folder","title":"Code Walkthrough","text":"","code":"# Get path to public catalog .db_catalog_path <- function() {   file.path(.db_get(\"data_path\"), \"_catalog\") }  # Publish metadata to public catalog .db_publish_metadata <- function(section, dataset) {   source_path <- .db_metadata_path(section, dataset)   dest_path <- .db_public_metadata_path(section, dataset)    # Copy metadata with public flag   metadata <- .db_read_metadata(source_path)   metadata$public <- TRUE   metadata$catalog_published_at <- Sys.time()    dir.create(dirname(dest_path), recursive = TRUE, showWarnings = FALSE)   jsonlite::write_json(metadata, dest_path, pretty = TRUE, auto_unbox = TRUE) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"ducklake-mode-master-sqlite-catalog","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary > Public Catalog Functions","what":"DuckLake Mode: Master SQLite Catalog","title":"Code Walkthrough","text":"","code":"# Connect to master catalog .db_master_connect <- function(master_path = NULL) {   if (is.null(master_path)) {     master_path <- .db_master_path()  # From option or default   }   DBI::dbConnect(RSQLite::SQLite(), master_path) }  # Publish to master catalog .db_publish_to_master <- function(schema, table) {   section <- .db_get(\"section\")   master_con <- .db_master_connect()    # Get column info and docs from DuckLake catalog   cols <- DBI::dbGetQuery(con, \"SELECT ... FROM information_schema.columns\")   docs <- db_get_docs(schema = schema, table = table)    # Upsert to master   DBI::dbExecute(master_con, \"     INSERT OR REPLACE INTO tables     (section_name, schema_name, table_name, description, owner, ...)     VALUES (?, ?, ?, ?, ?, ...)   \") }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"unified-api-pattern","dir":"Articles","previous_headings":"R/docs.R - Documentation & Data Dictionary","what":"Unified API Pattern","title":"Code Walkthrough","text":"modes use user-facing functions: Key points: API regardless mode Mode detection determines backend use db_describe(public = TRUE) integrates publishing documentation","code":"db_set_public <- function(section = NULL, dataset = NULL,                           schema = \"main\", table = NULL) {   curr_mode <- .db_get(\"mode\")    if (curr_mode == \"hive\") {     .db_publish_metadata(section, dataset)   } else {     .db_publish_to_master(schema, table)   } }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"rdb_connect-r---section-management","dir":"Articles","previous_headings":"","what":"R/db_connect.R - Section Management","title":"Code Walkthrough","text":"multi-catalog DuckLake, sections registered master catalog: Key points: Master catalog SQLite (read everyone, write admins) db_lake_connect_section() looks paths automatically Section stored .db_env enables db_describe(public=TRUE) sync","code":"db_register_section <- function(section, catalog_path, data_path, ...) {   master_con <- .db_master_connect()    # Ensure schema exists   .db_ensure_master_schema(master_con)    # Upsert section   DBI::dbExecute(master_con, \"     INSERT OR REPLACE INTO sections     (section_name, catalog_path, data_path, ...)     VALUES (?, ?, ?, ...)   \") }  db_lake_connect_section <- function(section, master_path = NULL, ...) {   # Look up section in master   master_con <- DBI::dbConnect(RSQLite::SQLite(), master_path)   section_info <- DBI::dbGetQuery(master_con, \"     SELECT catalog_path, data_path FROM sections WHERE section_name = ?   \")    # Connect using looked-up paths   con <- db_lake_connect(     metadata_path = section_info$catalog_path,     data_path = section_info$data_path,     ...   )    # Store section for master catalog sync   assign(\"section\", section, envir = .db_env) }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_preview_hive_write---preview-before-writing","dir":"Articles","previous_headings":"R/preview.R - Write Previews","what":"db_preview_hive_write() - Preview Before Writing","title":"Code Walkthrough","text":"Key points: Gathers relevant information without making changes Compares incoming schema existing schema replace_partitions, shows folders affected Returns structured data programmatic use, prints human-readable summary","code":"db_preview_hive_write <- function(data, section, dataset,                                    partition_by = NULL,                                    mode = c(\"overwrite\", \"append\", \"ignore\", \"replace_partitions\")) {      preview <- list(     mode = mode,     target_exists = dir.exists(output_path),     incoming = list(       rows = nrow(data),       cols = ncol(data),       columns = names(data)     )   )      # If target exists, compare schemas   if (preview$target_exists) {     existing_schema <- DBI::dbGetQuery(con, \"DESCRIBE SELECT * FROM read_parquet(...)\")          preview$schema_changes <- list(       new_columns = setdiff(names(data), existing_schema$name),       removed_columns = setdiff(existing_schema$name, names(data)),       type_changes = ...     )   }      # For partitioned writes, show impact   if (!is.null(partition_by)) {     part_vals <- unique(data[partition_by])     preview$partition_impact <- list(       partitions_in_data = nrow(part_vals),       existing_partitions_to_replace = ...     )   }      .print_hive_preview(preview)   invisible(preview) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_preview_upsert---preview-insert-vs-update-counts","dir":"Articles","previous_headings":"R/preview.R - Write Previews","what":"db_preview_upsert() - Preview Insert vs Update Counts","title":"Code Walkthrough","text":"","code":"db_preview_upsert <- function(data, schema, table, by, update_cols = NULL) {      # Register data temporarily   tmp <- .db_temp_name()   duckdb::duckdb_register(con, tmp, data)      # Count matches (updates)   match_sql <- glue::glue(\"     SELECT COUNT(*) FROM {tmp} s     INNER JOIN {qname} t ON {join_on_keys}   \")   matches <- DBI::dbGetQuery(con, match_sql)$n      # Check for duplicate keys   dup_sql <- glue::glue(\"     SELECT {key_cols}, COUNT(*) FROM {tmp}     GROUP BY {key_cols} HAVING COUNT(*) > 1   \")   dups <- DBI::dbGetQuery(con, dup_sql)      preview$impact <- list(     inserts = nrow(data) - matches,     updates = matches,     duplicates_in_incoming = nrow(dups)   )      .print_upsert_preview(preview)   invisible(preview) }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"hive-discovery","dir":"Articles","previous_headings":"R/discovery.R - Finding Data","what":"Hive Discovery","title":"Code Walkthrough","text":"Key points: Uses file system operations (list.dirs) Filters hidden folders (.git, etc.) partition folders (year=2024)","code":"db_list_sections <- function() {   base_path <- .db_get(\"data_path\")      # List directories (sections are top-level folders)   all_items <- list.dirs(base_path, full.names = FALSE, recursive = FALSE)      # Filter out hidden folders   all_items[!grepl(\"^\\\\.\", all_items)] }  db_list_datasets <- function(section) {   section_path <- file.path(base_path, section)   all_items <- list.dirs(section_path, full.names = FALSE, recursive = FALSE)      # Filter out partition folders (contain '=')   all_items[!grepl(\"=\", all_items)] }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"ducklake-discovery","dir":"Articles","previous_headings":"R/discovery.R - Finding Data","what":"DuckLake Discovery","title":"Code Walkthrough","text":"Key points: Uses standard information_schema views Filters catalog avoid confusion attached databases","code":"db_list_tables <- function(schema = \"main\") {   sql <- glue::glue(\"     SELECT table_name     FROM information_schema.tables     WHERE table_catalog = {.db_sql_quote(catalog)}       AND table_schema  = {.db_sql_quote(schema)}       AND table_type    = 'BASE TABLE'   \")   DBI::dbGetQuery(con, sql)$table_name }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_vacuum---clean-up-old-snapshots","dir":"Articles","previous_headings":"R/maintenance.R - Admin Operations","what":"db_vacuum() - Clean Up Old Snapshots","title":"Code Walkthrough","text":"Key points: dry_run = TRUE default prevents accidental deletion Shows clear /summary","code":"db_vacuum <- function(older_than = \"30 days\", dry_run = TRUE) {   # Get snapshots before   snapshots_before <- db_snapshots()      if (dry_run) {     # Calculate what would be removed     # Show preview     return(invisible(to_remove))   }      # Actually vacuum   DBI::dbExecute(con, glue::glue(     \"CALL ducklake_vacuum({.db_sql_quote(catalog)}, INTERVAL '30 days')\"   )) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_rollback---restore-previous-version","dir":"Articles","previous_headings":"R/maintenance.R - Admin Operations","what":"db_rollback() - Restore Previous Version","title":"Code Walkthrough","text":"Key points: Rollback creates NEW snapshot old data (non-destructive) can always â€œroll forwardâ€ rolling back later version","code":"db_rollback <- function(schema, table, version = NULL, timestamp = NULL) {   # Build time travel clause   at_clause <- glue::glue(\"AT (VERSION => {version})\")      # Rollback = create new version with old data   rollback_sql <- glue::glue(     \"CREATE OR REPLACE TABLE {qname} AS SELECT * FROM {qname} {at_clause}\"   )      DBI::dbExecute(con, rollback_sql) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"db_diff---compare-versions","dir":"Articles","previous_headings":"R/maintenance.R - Admin Operations","what":"db_diff() - Compare Versions","title":"Code Walkthrough","text":"Key points: Uses SQL EXCEPT set difference operations key_cols, can distinguish â€œmodifiedâ€ â€œadded/removedâ€","code":"db_diff <- function(schema, table, from_version, to_version, key_cols = NULL) {   # ADDED: rows in 'to' but not in 'from'   added_sql <- glue::glue(\"SELECT * FROM {to_ref} EXCEPT SELECT * FROM {from_ref}\")      # REMOVED: rows in 'from' but not in 'to'   removed_sql <- glue::glue(\"SELECT * FROM {from_ref} EXCEPT SELECT * FROM {to_ref}\")      result <- list(     added = DBI::dbGetQuery(con, added_sql),     removed = DBI::dbGetQuery(con, removed_sql)   )      # If key_cols provided, find modified rows (same key, different values)   if (!is.null(key_cols)) {     # Modified = key appears in both added and removed     modified_sql <- glue::glue(\"       SELECT new_tbl.*        FROM ({added_sql}) AS new_tbl       INNER JOIN ({removed_sql}) AS old_tbl       ON {join_on_keys}     \")     result$modified <- DBI::dbGetQuery(con, modified_sql)   }      result }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"rzzz-r---package-hooks","dir":"Articles","previous_headings":"","what":"R/zzz.R - Package Hooks","title":"Code Walkthrough","text":"Key points: .onAttach runs package loaded (shows helpful message) .onUnload ensures cleanup package unloaded %||% handles NULL zero-length values","code":"# Null coalesce operator used throughout `%||%` <- function(x, y) if (is.null(x) || length(x) == 0) y else x  .onAttach <- function(libname, pkgname) {   packageStartupMessage(     \"datapond \", utils::packageVersion(\"datapond\"), \"\\n\",     \"Use db_connect() for hive mode or db_lake_connect() for DuckLake mode.\"   ) }  .onUnload <- function(libpath) {   # Clean up connection when package unloads   con <- .db_get_con()   if (!is.null(con)) {     try(DBI::dbDisconnect(con, shutdown = TRUE), silent = TRUE)   } }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"rbrowser-r-rbrowser_ui-r-rbrowser_server-r---interactive-browser","dir":"Articles","previous_headings":"","what":"R/browser.R, R/browser_ui.R, R/browser_server.R - Interactive Browser","title":"Code Walkthrough","text":"files implement Shiny gadget interactively browsing data lake.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"architecture","dir":"Articles","previous_headings":"R/browser.R, R/browser_ui.R, R/browser_server.R - Interactive Browser","what":"Architecture","title":"Code Walkthrough","text":"browser uses Shiny module pattern, allows : 1. Used standalone via db_browser() 2. Embedded Shiny apps via db_browser_ui() + db_browser_server()","code":"# Standalone usage db_browser()  # Embedded in another app ui <- fluidPage(   db_browser_ui(\"my_browser\") ) server <- function(input, output, session) {   db_browser_server(\"my_browser\") }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"browser-r---launcher","dir":"Articles","previous_headings":"R/browser.R, R/browser_ui.R, R/browser_server.R - Interactive Browser","what":"browser.R - Launcher","title":"Code Walkthrough","text":"","code":"db_browser <- function(height = \"500px\", viewer = c(\"dialog\", \"browser\", \"pane\")) {   # Check packages are available   .db_assert_browser_packages()      # Build app   ui <- db_browser_app_ui(height = height)   server <- function(input, output, session) {     db_browser_server(id = \"db_browser\", height = height)   }   app <- shiny::shinyApp(ui = ui, server = server)      # Run as gadget   shiny::runGadget(app, viewer = viewer_func, stopOnCancel = TRUE) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"browser_ui-r---module-ui","dir":"Articles","previous_headings":"R/browser.R, R/browser_ui.R, R/browser_server.R - Interactive Browser","what":"browser_ui.R - Module UI","title":"Code Walkthrough","text":"UI built bslib modern Bootstrap 5 styling: Key points: NS(id) creates namespaced IDs multiple instances donâ€™t conflict bslib provides modern Bootstrap 5 components Tree view rendered dynamically based hive vs DuckLake mode","code":"db_browser_ui <- function(id, height = \"500px\") {   ns <- shiny::NS(id)  # Namespace for module      # Sidebar with tree view   sidebar <- bslib::sidebar(     shiny::uiOutput(ns(\"tree_view\")),     shiny::actionButton(ns(\"refresh_tree\"), \"Refresh\")   )      # Main content with tabs   main_content <- bslib::navset_card_tab(     bslib::nav_panel(\"Preview\", ...),     bslib::nav_panel(\"Metadata\", ...),     bslib::nav_panel(\"Search\", ...),     bslib::nav_panel(\"Dictionary\", ...)   )      bslib::page_sidebar(sidebar = sidebar, main_content) }"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"browser_server-r---module-server","dir":"Articles","previous_headings":"R/browser.R, R/browser_ui.R, R/browser_server.R - Interactive Browser","what":"browser_server.R - Module Server","title":"Code Walkthrough","text":"Key points: moduleServer() creates namespaced server logic reactiveValues tracks UI state (selected table, etc.) eventReactive triggers expensive operations button click DT::datatable provides interactive tables filtering","code":"db_browser_server <- function(id, height = \"500px\") {   shiny::moduleServer(id, function(input, output, session) {          # Reactive values for selection state     rv <- shiny::reactiveValues(       selected_section = NULL,       selected_dataset = NULL     )          # Tree view - renders differently for hive vs DuckLake     output$tree_view <- shiny::renderUI({       if (is_hive) .render_hive_tree(ns, rv)       else .render_ducklake_tree(ns, rv)     })          # Preview - loads data on button click     preview_data <- shiny::eventReactive(input$load_preview, {       db_hive_read(rv$selected_section, rv$selected_dataset) |>         head(input$preview_rows) |>         collect()     })          output$preview_table <- DT::renderDataTable({       DT::datatable(preview_data(), filter = \"top\")     })          # Search     search_results <- shiny::eventReactive(input$do_search, {       db_search(input$search_pattern, field = input$search_field)     })          # Dictionary with download     output$download_dict <- shiny::downloadHandler(       filename = \"data_dictionary.csv\",       content = function(file) {         write.csv(db_dictionary(), file)       }     )   }) }"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"sql-injection-prevention","dir":"Articles","previous_headings":"Security Considerations","what":"SQL Injection Prevention","title":"Code Walkthrough","text":"user inputs go SQL either: Validated .db_validate_name() (allows -Za-z0-9_-) Quoted .db_sql_quote() (escapes single quotes)","code":"# This is safe: section <- .db_validate_name(section)  # Rejects \"../../../etc\" path <- .db_sql_quote(glob_path)       # Escapes quotes properly"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"path-traversal-prevention","dir":"Articles","previous_headings":"Security Considerations","what":"Path Traversal Prevention","title":"Code Walkthrough","text":".db_validate_name() rejects input containing: - Path separators (/, \\) - Parent directory references (..) - Special characters prevents attacks like:","code":"db_hive_read(\"../../../etc\", \"passwd\")  # Rejected!"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"testing-your-understanding","dir":"Articles","previous_headings":"","what":"Testing Your Understanding","title":"Code Walkthrough","text":"Try answering questions: use .db_get_con() instead accessing .db_env$con directly? happens db_lake_write() fails halfway ? db_hive_read() return lazy table instead collected data? replace_partitions mode know folders delete? â€™s difference update_cols = NULL update_cols = character(0) db_upsert()? choose SQLite DuckDB catalog backend? db_preview_upsert() know many rows updated vs inserted? documentation metadata stored hive mode vs DuckLake mode?","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/code-walkthrough.html","id":"contributing-to-the-package","dir":"Articles","previous_headings":"","what":"Contributing to the Package","title":"Code Walkthrough","text":"adding new functions: Always validate inputs .db_validate_name() custom validation Check connection mode start every function Use .db_get() .db_get_con() accessing stored state Return invisibly side-effect functions, visibly queries Add roxygen documentation @param, @return, @examples, @export Update NAMESPACE adding new exports Add tests tests/testthat/ new functionality Run devtools::document() adding roxygen comments regenerate NAMESPACE help files.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"the-problem-were-solving","dir":"Articles","previous_headings":"","what":"The Problem Weâ€™re Solving","title":"Data Lake Concepts","text":"Within organisation, different sections need share data: Trade produces import/export figures National Accounts needs Labour produces employment data multiple sections consume Health produces statistics feed analyses Currently, happens shared network drives folder-based access control. works, limitations: versioning (someone overwrites file, old version gone) way know data changed changed Large files slow query (must read whole thing) schema enforcement (columns can change without warning) datapond addresses issues keeping familiar folder--permissions model already supports.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"what-is-a-data-lake","dir":"Articles","previous_headings":"","what":"What is a Data Lake?","title":"Data Lake Concepts","text":"data lake storage system designed hold large amounts structured data files (typically Parquet format) rather traditional database. Parquet? Parquet columnar file format â€™s: Compressed - files 5-10x smaller CSV Fast - reads columns need Typed - preserves data types (dates, numbers, strings) Universal - works R, Python, SAS, Excel, ","code":"Traditional Database          Data Lake â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Database       â”‚          â”‚  Files on disk  â”‚ â”‚  Server         â”‚          â”‚  (Parquet)      â”‚ â”‚                 â”‚          â”‚                 â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚  ğŸ“ Trade/      â”‚ â”‚  â”‚ Table A   â”‚  â”‚          â”‚    ğŸ“„ data.parquet â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚          â”‚  ğŸ“ Labour/     â”‚ â”‚  â”‚ Table B   â”‚  â”‚          â”‚    ğŸ“„ data.parquet â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚                 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â†“                              â†“   Needs server               Just files!   Needs DBA                  Query with DuckDB   Licensing costs            Free & fast"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"hive-partitioning-explained","dir":"Articles","previous_headings":"","what":"Hive Partitioning Explained","title":"Data Lake Concepts","text":"Hive partitioning way organising files folders based column values. folder names encode data contain.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"example-trade-imports-data","dir":"Articles","previous_headings":"Hive Partitioning Explained","what":"Example: Trade Imports Data","title":"Data Lake Concepts","text":"Instead one massive file: partition year month:","code":"Trade/Imports/all_data.parquet  (10 GB - slow to read!) Trade/Imports/   year=2023/     month=01/       data.parquet       data_part2.parquet     month=02/       data.parquet     ...   year=2024/     month=01/       data.parquet     ..."},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"why-this-matters","dir":"Articles","previous_headings":"Hive Partitioning Explained","what":"Why This Matters","title":"Data Lake Concepts","text":"query January 2024 data: DuckDB smart enough read year=2024/month=01/ folder. skips folders entirely. called partition pruning makes queries large datasets dramatically faster.","code":"imports |>    filter(year == 2024, month == 1)"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"choosing-partition-columns","dir":"Articles","previous_headings":"Hive Partitioning Explained","what":"Choosing Partition Columns","title":"Data Lake Concepts","text":"Good partition columns : Low cardinality - many unique values (year, month, region) Frequently filtered - columns often use clauses Stable - values donâ€™t change existing records Bad partition columns: High cardinality - unique IDs, timestamps seconds Rarely filtered - columns never filter Rule thumb: Aim partition folders containing 100MB - 1GB data .","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"how-datapond-uses-hive-partitioning","dir":"Articles","previous_headings":"Hive Partitioning Explained","what":"How datapond Uses Hive Partitioning","title":"Data Lake Concepts","text":"","code":"# Write with partitioning db_hive_write(   my_data,   section = \"Trade\",   dataset = \"Imports\",   partition_by = c(\"year\", \"month\")  # Creates year=X/month=Y folders )  # Read - partitions are automatic columns imports <- db_hive_read(\"Trade\", \"Imports\") imports |> filter(year == 2024)  # Only reads 2024 folders"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"what-is-ducklake","dir":"Articles","previous_headings":"","what":"What is DuckLake?","title":"Data Lake Concepts","text":"DuckLake newer approach adds database-like features top Parquet files. Think â€œParquet files superpowersâ€.","code":"Hive Partitioning              DuckLake â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Just files      â”‚          â”‚ Files +         â”‚ â”‚                 â”‚          â”‚ Metadata catalogâ”‚ â”‚ ğŸ“ year=2024/   â”‚          â”‚                 â”‚ â”‚   ğŸ“„ data.parquet          â”‚ ğŸ“„ catalog.sqlite â”‚                 â”‚          â”‚ ğŸ“ data/        â”‚ â”‚ No versioning   â”‚          â”‚   ğŸ“„ file1.parquet â”‚ No transactions â”‚          â”‚   ğŸ“„ file2.parquet â”‚                 â”‚          â”‚                 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ âœ… Versioning   â”‚                              â”‚ âœ… Transactions â”‚                              â”‚ âœ… Time travel  â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"the-metadata-catalog","dir":"Articles","previous_headings":"What is DuckLake?","what":"The Metadata Catalog","title":"Data Lake Concepts","text":"DuckLake needs place store metadata tables. catalog tracks: Parquet files belong table schema (columns types) table history changes (snapshots) made changes actual data still Parquet files - DuckLake just adds management layer.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"choosing-a-catalog-backend","dir":"Articles","previous_headings":"","what":"Choosing a Catalog Backend","title":"Data Lake Concepts","text":"DuckLake supports three different backends storing catalog metadata:","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"duckdb-single-user","dir":"Articles","previous_headings":"Choosing a Catalog Backend","what":"1. DuckDB (Single User)","title":"Data Lake Concepts","text":"Metadata stored .ducklake file Single client - two people connect, one fail Good : personal use, development, testing","code":"db_lake_connect(   catalog_type = \"duckdb\",   metadata_path = \"metadata.ducklake\",   data_path = \"//CSO-NAS/DataLake/data\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"sqlite-multiple-local-users---recommended","dir":"Articles","previous_headings":"Choosing a Catalog Backend","what":"2. SQLite (Multiple Local Users) - RECOMMENDED","title":"Data Lake Concepts","text":"Metadata stored .sqlite file network drive Multiple readers + single writer automatic retry Still just file - server needed Good : shared network drives, use cases SQLite handles concurrency: someone writing, writers wait retry automatically. Readers can continue uninterrupted. works well typical usage writes less frequent reads.","code":"db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"postgresql-multi-user-lakehouse","dir":"Articles","previous_headings":"Choosing a Catalog Backend","what":"3. PostgreSQL (Multi-User Lakehouse)","title":"Data Lake Concepts","text":"Metadata stored PostgreSQL database Full concurrent access - multiple readers writers Requires PostgreSQL 12+ server Good : large teams, high write concurrency, remote access","code":"db_lake_connect(   catalog_type = \"postgres\",   metadata_path = \"dbname=ducklake_catalog host=db.cso.ie\",   data_path = \"//CSO-NAS/DataLake/data\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"which-should-you-choose","dir":"Articles","previous_headings":"Choosing a Catalog Backend","what":"Which Should You Choose?","title":"Data Lake Concepts","text":"use cases, start SQLite. â€™s still just file (familiar, works permissions), handles multiple users gracefully.","code":"Are you the only user?   â””â”€ Yes â†’ DuckDB (simplest)   â””â”€ No â†’ Are you on a shared network drive?             â””â”€ Yes â†’ SQLite (recommended)             â””â”€ No â†’ Do you need high write concurrency?                       â””â”€ Yes â†’ PostgreSQL                       â””â”€ No â†’ SQLite"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"time-travel","dir":"Articles","previous_headings":"Key DuckLake Features","what":"1. Time Travel","title":"Data Lake Concepts","text":"Query data existed point past: invaluable : Someone reports â€œnumbers looked different yesterdayâ€ - can check! need reproduce analysis specific date Something went wrong need see /","code":"# Current data products <- db_lake_read(table = \"products\")  # Data as of version 5 products_v5 <- db_lake_read(table = \"products\", version = 5)  # Data as of last Tuesday products_tue <- db_lake_read(table = \"products\",                                timestamp = \"2025-01-14 00:00:00\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"snapshots-and-audit-trail","dir":"Articles","previous_headings":"Key DuckLake Features","what":"2. Snapshots and Audit Trail","title":"Data Lake Concepts","text":"Every change creates new snapshot metadata: always know changed, , (recorded) .","code":"db_snapshots() #>   snapshot_id snapshot_time       commit_author commit_message #> 1           1 2025-01-01 09:00:00 jsmith        Initial load #> 2           2 2025-01-15 14:30:00 mjones        Added Q4 data #> 3           3 2025-01-20 11:00:00 mjones        Fixed country codes"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"acid-transactions","dir":"Articles","previous_headings":"Key DuckLake Features","what":"3. ACID Transactions","title":"Data Lake Concepts","text":"Changes atomic - either fully succeed fully fail. partial updates leave data broken state.","code":"# If this fails halfway through, no data is changed db_lake_write(big_dataset, table = \"imports\", mode = \"overwrite\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"schema-evolution","dir":"Articles","previous_headings":"Key DuckLake Features","what":"4. Schema Evolution","title":"Data Lake Concepts","text":"DuckLake handles schema changes gracefully: Add new column? Old data gets NULLs column Query old versions? still work old schema","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"schemas-organising-tables","dir":"Articles","previous_headings":"","what":"Schemas: Organising Tables","title":"Data Lake Concepts","text":"DuckLake, schemas like folders tables. help organise related tables together.","code":"catalog (datapond) â”œâ”€â”€ main (default schema) â”‚   â””â”€â”€ reference_tables â”œâ”€â”€ trade â”‚   â”œâ”€â”€ imports â”‚   â”œâ”€â”€ exports â”‚   â””â”€â”€ products â”œâ”€â”€ labour â”‚   â”œâ”€â”€ employment â”‚   â””â”€â”€ earnings â””â”€â”€ health     â”œâ”€â”€ hospitals     â””â”€â”€ waiting_times"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"using-schemas","dir":"Articles","previous_headings":"Schemas: Organising Tables","what":"Using Schemas","title":"Data Lake Concepts","text":"","code":"# Create a schema for your section db_create_schema(\"trade\")  # Write to it db_lake_write(imports_data, schema = \"trade\", table = \"imports\")  # Read from it imports <- db_lake_read(schema = \"trade\", table = \"imports\")  # List tables in a schema db_list_tables(\"trade\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"schemas-vs-sections","dir":"Articles","previous_headings":"Schemas: Organising Tables","what":"Schemas vs Sections","title":"Data Lake Concepts","text":"serve organisational purpose - grouping related data together.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"data-documentation","dir":"Articles","previous_headings":"","what":"Data Documentation","title":"Data Lake Concepts","text":"Good data governance requires documentation. datapond provides built-tools document datasets generate data dictionary.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"documenting-datasets","dir":"Articles","previous_headings":"Data Documentation","what":"Documenting Datasets","title":"Data Lake Concepts","text":"","code":"# Add metadata to a dataset db_describe(   section = \"Trade\",   dataset = \"Imports\",   description = \"Monthly import values by country and HS commodity code\",   owner = \"Trade Section\",   tags = c(\"trade\", \"monthly\", \"official\") )  # Document individual columns db_describe_column(   section = \"Trade\",   dataset = \"Imports\",   column = \"value\",   description = \"Import value\",   units = \"EUR (thousands)\" )  db_describe_column(   section = \"Trade\",   dataset = \"Imports\",   column = \"country_code\",   description = \"ISO 3166-1 alpha-2 country code\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"where-is-documentation-stored","dir":"Articles","previous_headings":"Data Documentation","what":"Where is Documentation Stored?","title":"Data Lake Concepts","text":"Hive mode: Creates _metadata.json file dataset folder DuckLake mode: Stores _metadata schema catalog Either way, documentation travels data.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"searching-and-discovery","dir":"Articles","previous_headings":"Data Documentation","what":"Searching and Discovery","title":"Data Lake Concepts","text":"","code":"# Search datasets by any field db_search(\"trade\")                        # Matches name, description, owner, or tags db_search(\"monthly\", field = \"tags\")      # Search only tags db_search(\"Trade Section\", field = \"owner\")  # Find datasets by owner  # Find columns across all datasets db_search_columns(\"country\") #>   section  dataset  column_name  column_type  column_description #> 1 Trade    Imports  country_code VARCHAR      ISO 3166-1 alpha-2... #> 2 Trade    Exports  country_code VARCHAR      ISO 3166-1 alpha-2... #> 3 Labour   Survey   country      VARCHAR      Country of residence"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"generating-a-data-dictionary","dir":"Articles","previous_headings":"Data Documentation","what":"Generating a Data Dictionary","title":"Data Lake Concepts","text":"data dictionary includes: - Dataset/table name location - Description, owner, tags - Column names, types, documentation - Last updated timestamps","code":"# Full data dictionary with column details dict <- db_dictionary()  # Just dataset-level summary dict_summary <- db_dictionary(include_columns = FALSE)  # Filter to specific section dict_trade <- db_dictionary(section = \"Trade\")  # Export to Excel writexl::write_xlsx(dict, \"data_dictionary.xlsx\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"preview-before-writing","dir":"Articles","previous_headings":"","what":"Preview Before Writing","title":"Data Lake Concepts","text":"making changes production data, can preview happen:","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"hive-write-preview","dir":"Articles","previous_headings":"Preview Before Writing","what":"Hive Write Preview","title":"Data Lake Concepts","text":"Shows: - Target path whether exists - Incoming vs existing row counts - Schema changes - new columns, removed columns, type changes - Partition impact - partitions affected","code":"db_preview_hive_write(   my_data,   section = \"Trade\",   dataset = \"Imports\",   partition_by = c(\"year\", \"month\"),   mode = \"replace_partitions\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"ducklake-write-preview","dir":"Articles","previous_headings":"Preview Before Writing","what":"DuckLake Write Preview","title":"Data Lake Concepts","text":"Shows: - Current vs new row counts - Schema comparison - Warnings (e.g., append non-existent table)","code":"db_preview_lake_write(my_data, table = \"imports\", mode = \"overwrite\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"upsert-preview","dir":"Articles","previous_headings":"Preview Before Writing","what":"Upsert Preview","title":"Data Lake Concepts","text":"Shows: - many rows inserted (new keys) - many rows updated (existing keys) - Warnings duplicate keys incoming data","code":"db_preview_upsert(my_data, table = \"products\", by = \"product_id\")"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"when-to-use-hive-mode","dir":"Articles","previous_headings":"Comparing the Two Approaches","what":"When to Use Hive Mode","title":"Data Lake Concepts","text":"âœ… Good : Migrating existing SAS folder structure Simple data sharing without versioning needs need direct file access (tools canâ€™t use DuckLake) Getting started quickly minimal setup âŒ Limitations: versioning - overwrites permanent transaction safety - partial failures can corrupt data Manual schema management","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"when-to-use-ducklake-mode","dir":"Articles","previous_headings":"Comparing the Two Approaches","what":"When to Use DuckLake Mode","title":"Data Lake Concepts","text":"âœ… Good : Production data pipelines reliability matters need audit trails (changed , ) Analyses need reproducible specific points time Complex updates (upserts, merges) âŒ Considerations: Slightly complex setup (choose catalog backend) Newer technology (less battle-tested simple files)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"access-control","dir":"Articles","previous_headings":"","what":"Access Control","title":"Data Lake Concepts","text":"modes use file system permissions - model commonly used.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"hive-mode-access","dir":"Articles","previous_headings":"Access Control","what":"Hive Mode Access","title":"Data Lake Concepts","text":"Users get access granted folder permissions .","code":"//CSO-NAS/DataLake/ â”œâ”€â”€ Trade/          â† Trade section has read/write â”‚   â”œâ”€â”€ Imports/ â”‚   â””â”€â”€ Exports/ â”œâ”€â”€ Labour/         â† Labour section has read/write â”‚   â””â”€â”€ Employment/ â””â”€â”€ Shared/         â† Everyone has read access     â””â”€â”€ Reference/"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"ducklake-mode-access","dir":"Articles","previous_headings":"Access Control","what":"DuckLake Mode Access","title":"Data Lake Concepts","text":"Read access = can query data Write access = can modify data permissions work file/folder level, just like . PostgreSQL catalogs, â€™d also need database credentials - Parquet data files still use file system permissions.","code":"//CSO-NAS/DataLake/ â”œâ”€â”€ catalog.sqlite      â† Need read (or read/write) access â””â”€â”€ data/               â† Need read (or read/write) access     â”œâ”€â”€ trade/     â””â”€â”€ labour/"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"public-catalog-organisation-wide-discovery","dir":"Articles","previous_headings":"","what":"Public Catalog: Organisation-wide Discovery","title":"Data Lake Concepts","text":"common challenge: people discover data exists without access ? Hive mode, canâ€™t access Trade folder, canâ€™t see datasets Trade . datapond solves public catalog - shared location metadata .","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"how-it-works","dir":"Articles","previous_headings":"Public Catalog: Organisation-wide Discovery","what":"How It Works","title":"Data Lake Concepts","text":"","code":"//CSO-NAS/DataLake/ â”œâ”€â”€ _catalog/             â† Everyone can read this folder â”‚   â”œâ”€â”€ Trade/ â”‚   â”‚   â”œâ”€â”€ Imports.json  â† Metadata only (description, owner, columns) â”‚   â”‚   â””â”€â”€ Exports.json â”‚   â””â”€â”€ Labour/ â”‚       â””â”€â”€ Employment.json â”œâ”€â”€ Trade/                â† Only Trade section can access â”‚   â””â”€â”€ Imports/ â”‚       â”œâ”€â”€ _metadata.json â† Source metadata â”‚       â””â”€â”€ year=2024/ â”‚           â””â”€â”€ data.parquet â””â”€â”€ Labour/               â† Only Labour section can access     â””â”€â”€ Employment/"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"publishing-to-the-public-catalog","dir":"Articles","previous_headings":"Public Catalog: Organisation-wide Discovery","what":"Publishing to the Public Catalog","title":"Data Lake Concepts","text":"","code":"# Make a dataset discoverable db_describe(   section = \"Trade\",   dataset = \"Imports\",   description = \"Monthly import values by country\",   owner = \"Trade Section\",   public = TRUE  # Copies metadata to _catalog/Trade/Imports.json )  # Or use the convenience function db_set_public(section = \"Trade\", dataset = \"Imports\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"discovering-data","dir":"Articles","previous_headings":"Public Catalog: Organisation-wide Discovery","what":"Discovering Data","title":"Data Lake Concepts","text":"","code":"# Anyone can see what datasets exist (even without data access) db_list_public() #>   section dataset            description        owner #> 1   Trade Imports Monthly import values  Trade Section #> 2  Labour Employment    Employment stats Labour Section  # Find datasets by keyword db_search(\"monthly\", field = \"description\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"security-model","dir":"Articles","previous_headings":"Public Catalog: Organisation-wide Discovery","what":"Security Model","title":"Data Lake Concepts","text":"data stays secure - public catalog contains descriptions, actual data values.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"multi-catalog-ducklake","dir":"Articles","previous_headings":"","what":"Multi-Catalog DuckLake","title":"Data Lake Concepts","text":"larger organisations, section may DuckLake catalog. master discovery catalog provides organisation-wide visibility.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"architecture","dir":"Articles","previous_headings":"Multi-Catalog DuckLake","what":"Architecture","title":"Data Lake Concepts","text":"","code":"//CSO-NAS/DataLake/ â”œâ”€â”€ _master/ â”‚   â””â”€â”€ discovery.sqlite     â† Master catalog (everyone reads) â”œâ”€â”€ trade/ â”‚   â”œâ”€â”€ catalog.sqlite       â† Trade's DuckLake catalog â”‚   â””â”€â”€ data/                â† Trade's Parquet files â”œâ”€â”€ labour/ â”‚   â”œâ”€â”€ catalog.sqlite       â† Labour's DuckLake catalog â”‚   â””â”€â”€ data/ â””â”€â”€ shared/     â”œâ”€â”€ catalog.sqlite     â””â”€â”€ data/"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"setting-up-admin","dir":"Articles","previous_headings":"Multi-Catalog DuckLake","what":"Setting Up (Admin)","title":"Data Lake Concepts","text":"","code":"# Create master catalog db_setup_master(\"//CSO-NAS/DataLake/_master/discovery.sqlite\")  # Register each section db_register_section(   section = \"trade\",   catalog_path = \"//CSO-NAS/DataLake/trade/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/trade/data\",   owner = \"Trade Team\" )  db_register_section(   section = \"labour\",   catalog_path = \"//CSO-NAS/DataLake/labour/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/labour/data\",   owner = \"Labour Team\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"using-sections-user","dir":"Articles","previous_headings":"Multi-Catalog DuckLake","what":"Using Sections (User)","title":"Data Lake Concepts","text":"","code":"# Connect to a section (looks up paths from master) db_lake_connect_section(\"trade\")  # Work with data imports <- db_lake_read(table = \"imports\")  # Document and make discoverable (same API!) db_describe(   table = \"imports\",   description = \"Monthly import statistics\",   public = TRUE  # Syncs to master catalog )  # Discover across all sections db_list_public() #>   section schema    table           description #> 1   trade   main  imports Monthly import stats #> 2  labour   main employment    Employment data  # Switch to another section db_switch_section(\"labour\")"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"section-vs-schema-in-ducklake","dir":"Articles","previous_headings":"Multi-Catalog DuckLake","what":"Section vs Schema in DuckLake","title":"Data Lake Concepts","text":"DuckLake mode, section schema exist serve different purposes: | Concept | | Purpose | |â€”â€”â€”|â€”â€”â€”â€”|â€”â€”â€”| | Section | separate DuckLake catalog file | Ownership boundary - controls can write/publish | | Schema | namespace within catalog | Logical grouping related tables | section might contain multiple schemas: Key difference: - Section = owns (security boundary, controlled file permissions) - Schema = â€™s organised (logical boundary, access control) Trade Team can make trade.main.imports public trade/ section. schema just organisation within catalog.","code":"trade/catalog.sqlite          â† Section (owned by Trade Team) â”œâ”€â”€ main.imports              â† schema.table â”œâ”€â”€ main.exports â”œâ”€â”€ staging.imports_draft     â† Different schema for work-in-progress â””â”€â”€ reference.countries       â† Reference data schema"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"unified-api","dir":"Articles","previous_headings":"Multi-Catalog DuckLake","what":"Unified API","title":"Data Lake Concepts","text":"functions work modes:","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"putting-it-together","dir":"Articles","previous_headings":"","what":"Putting It Together","title":"Data Lake Concepts","text":"â€™s typical workflow might look:","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"publishing-data-producer","dir":"Articles","previous_headings":"Putting It Together","what":"Publishing Data (Producer)","title":"Data Lake Concepts","text":"","code":"library(datapond)  # Connect with SQLite catalog db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )  # Prepare your data imports_q1 <- prepare_imports_data(raw_files)  # Preview what will happen db_preview_lake_write(imports_q1, schema = \"trade\", table = \"imports\", mode = \"append\")  # Publish with a meaningful commit message db_lake_write(   imports_q1,   schema = \"trade\",   table = \"imports\",   mode = \"append\",   commit_author = Sys.info()[\"user\"],   commit_message = \"Q1 2025 imports data - final\" )  # Document your dataset db_describe(   schema = \"trade\",   table = \"imports\",   description = \"Monthly import values by country and HS code\",   owner = \"Trade Section\" )  db_disconnect()"},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"consuming-data-consumer","dir":"Articles","previous_headings":"Putting It Together","what":"Consuming Data (Consumer)","title":"Data Lake Concepts","text":"","code":"library(datapond)  # Connect (read-only access is fine) db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )  # Discover what's available db_list_schemas() db_list_tables(\"trade\")  # Search for relevant data db_search(\"imports\")  # Check documentation db_get_docs(schema = \"trade\", table = \"imports\")  # Read and analyse imports <- db_lake_read(schema = \"trade\", table = \"imports\")  imports |>   filter(year == 2025, quarter == 1) |>   summarise(total_value = sum(value)) |>   collect()  db_disconnect()"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/articles/concepts.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Data Lake Concepts","text":"See vignette(\"code-walkthrough\") detailed explanation package code works Try examples README get hands-experience Start Hive mode migrating SAS, move DuckLake ready DuckLake, use SQLite catalog shared drives","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Cathal Byrne. Author, maintainer.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Byrne C (2026). datapond: Local Data Lake Infrastructure R. R package version 0.1.0, https://github.com/CathalByrneGit/datapond.","code":"@Manual{,   title = {datapond: Local Data Lake Infrastructure for R},   author = {Cathal Byrne},   year = {2026},   note = {R package version 0.1.0},   url = {https://github.com/CathalByrneGit/datapond}, }"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"datapond","dir":"","previous_headings":"","what":"Local Data Lake Infrastructure for R","title":"Local Data Lake Infrastructure for R","text":"datapond simple ligthweight data lake infrastructure small medium data requirements. Provides unified R interface duckdb ducklake internal data infrastructure. supports two storage backends: Hive-partitioned Parquet - familiar folder-based structure (similar existing SAS storage) DuckLake - modern data lakehouse time travel, schema evolution, ACID transactions backends use DuckDB query engine, giving fast analytical queries without needing server.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Local Data Lake Infrastructure for R","text":"","code":"# Install from local source devtools::install(\"path/to/datapond\")  # Or load for development devtools::load_all(\"path/to/datapond\")"},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"hive-mode-folder-based","dir":"","previous_headings":"Quick Start","what":"Hive Mode (Folder-based)","title":"Local Data Lake Infrastructure for R","text":"","code":"library(datapond)  # Connect to the data lake db_connect(path = \"//CSO-NAS/DataLake\")  # See what's available db_list_sections() #> [1] \"Trade\" \"Labour\" \"Health\" \"Agriculture\"  db_list_datasets(\"Trade\") #> [1] \"Imports\" \"Exports\" \"Balance\"  # Read a dataset (returns a lazy dplyr table) imports <- db_hive_read(\"Trade\", \"Imports\")  # Work with it using dplyr imports |>   filter(year == 2024) |>   group_by(country) |>   summarise(total = sum(value)) |>   collect()  # Preview before writing (see what will happen) db_preview_hive_write(   my_data,   section = \"Trade\",   dataset = \"Imports\",   partition_by = c(\"year\", \"month\"),   mode = \"replace_partitions\" )  # Write data (partitioned by year and month) db_hive_write(   my_data,    section = \"Trade\",    dataset = \"Imports\",   partition_by = c(\"year\", \"month\"),   mode = \"replace_partitions\" )  # Disconnect when done db_disconnect()"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"ducklake-mode-time-travel","dir":"","previous_headings":"Quick Start","what":"DuckLake Mode (Time Travel)","title":"Local Data Lake Infrastructure for R","text":"","code":"library(datapond)  # Connect to DuckLake with SQLite catalog (recommended for shared drives) db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )  # See what's available db_list_schemas() #> [1] \"main\" \"trade\" \"labour\"  db_list_tables(\"trade\") #> [1] \"imports\" \"exports\" \"products\"  # Read current data imports <- db_lake_read(schema = \"trade\", table = \"imports\")  # Read data as it was at a specific version imports_v5 <- db_lake_read(schema = \"trade\", table = \"imports\", version = 5)  # Read data as it was at a specific time imports_jan <- db_lake_read(   schema = \"trade\",    table = \"imports\",    timestamp = \"2025-01-15 00:00:00\" )  # Preview write to see impact db_preview_lake_write(my_data, schema = \"trade\", table = \"imports\", mode = \"append\")  # Write with commit metadata db_lake_write(   my_data,   schema = \"trade\",   table = \"imports\",   mode = \"append\",   commit_author = \"jsmith\",   commit_message = \"Added Q1 2025 data\" )  # Preview upsert to see how many inserts vs updates db_preview_upsert(my_data, schema = \"trade\", table = \"products\", by = \"product_id\")  # Upsert (update existing, insert new) db_upsert(   my_data,   schema = \"trade\",   table = \"products\",   by = \"product_id\",   commit_message = \"Price updates\" )  # View snapshot history db_snapshots()  # Compare versions diff <- db_diff(schema = \"trade\", table = \"imports\",                  from_version = 5, to_version = 10) diff$added diff$removed  # Rollback if something went wrong db_rollback(schema = \"trade\", table = \"imports\", version = 5)  # Clean up old snapshots db_vacuum(older_than = \"30 days\", dry_run = FALSE)  db_disconnect()"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"data-documentation--discovery","dir":"","previous_headings":"Quick Start","what":"Data Documentation & Discovery","title":"Local Data Lake Infrastructure for R","text":"","code":"library(datapond) db_connect(path = \"//CSO-NAS/DataLake\")  # Document your datasets db_describe(   section = \"Trade\",   dataset = \"Imports\",   description = \"Monthly import values by country and commodity code\",   owner = \"Trade Section\",   tags = c(\"trade\", \"monthly\", \"official\") )  # Document individual columns db_describe_column(   section = \"Trade\",   dataset = \"Imports\",   column = \"value\",   description = \"Import value in thousands\",   units = \"EUR (thousands)\" )  # Search for datasets db_search(\"trade\") db_search(\"official\", field = \"tags\")  # Find columns across all datasets db_search_columns(\"country\")  # Generate a data dictionary dict <- db_dictionary() # Export to Excel writexl::write_xlsx(dict, \"data_dictionary.xlsx\")"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"public-catalog-organisation-wide-discovery","dir":"","previous_headings":"Quick Start","what":"Public Catalog (Organisation-wide Discovery)","title":"Local Data Lake Infrastructure for R","text":"Make metadata discoverable across organisation keeping data secure via folder permissions.","code":"library(datapond) db_connect(path = \"//CSO-NAS/DataLake\")  # Document and make discoverable in one step db_describe(   section = \"Trade\",   dataset = \"Imports\",   description = \"Monthly import values\",   owner = \"Trade Section\",   public = TRUE  # Copies metadata to shared _catalog/ folder )  # Or use convenience functions db_set_public(section = \"Trade\", dataset = \"Imports\") db_set_private(section = \"Trade\", dataset = \"Imports\") db_is_public(section = \"Trade\", dataset = \"Imports\")  # Anyone can discover what data exists (even without data access) db_list_public() #>   section dataset     description          owner #> 1   Trade Imports Monthly import values Trade Section #> 2  Labour Employment Employment statistics Labour Section  # Sync catalog after manual metadata changes db_sync_catalog()"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"multi-catalog-ducklake-enterprise-setup","dir":"","previous_headings":"Quick Start","what":"Multi-Catalog DuckLake (Enterprise Setup)","title":"Local Data Lake Infrastructure for R","text":"larger organisations multiple sections, use master discovery catalog:","code":"library(datapond)  # Admin: Set up master catalog (one-time) db_setup_master(\"//CSO-NAS/DataLake/_master/discovery.sqlite\")  # Register sections db_register_section(   section = \"trade\",   catalog_path = \"//CSO-NAS/DataLake/trade/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/trade/data\",   owner = \"Trade Team\" )  # User: Connect to a section db_lake_connect_section(\"trade\")  # Work with data imports <- db_lake_read(table = \"imports\")  # Document and publish (same API as Hive mode!) db_describe(   table = \"imports\",   description = \"Monthly import statistics\",   public = TRUE  # Syncs to master catalog )  # Discover all public tables across all sections db_list_public()  # Switch sections db_switch_section(\"labour\")"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"interactive-browser","dir":"","previous_headings":"Quick Start","what":"Interactive Browser","title":"Local Data Lake Infrastructure for R","text":"browser provides point--click interface : Browse - Navigate sections/datasets schemas/tables tree view Preview - View sample rows dataset Metadata - See documentation, owner, tags Search - Find datasets name, description, tags Dictionary - Generate export data dictionary","code":"library(datapond) db_connect(path = \"//CSO-NAS/DataLake\")  # Launch interactive browser db_browser()"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"why-two-modes","dir":"","previous_headings":"","what":"Why Two Modes?","title":"Local Data Lake Infrastructure for R","text":"Start Hive mode â€™re migrating SAS need something familiar. Use DuckLake mode need versioning, rollback, proper transaction support.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"choosing-a-catalog-backend-ducklake","dir":"","previous_headings":"","what":"Choosing a Catalog Backend (DuckLake)","title":"Local Data Lake Infrastructure for R","text":"DuckLake stores metadata (table definitions, snapshots, file tracking) catalog database. can choose three backends:","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"recommended-sqlite-for-cso","dir":"","previous_headings":"Choosing a Catalog Backend (DuckLake)","what":"Recommended: SQLite for CSO","title":"Local Data Lake Infrastructure for R","text":"CSO use cases shared network drives, use SQLite: SQLite? Still just file network drive (server needed) Supports multiple people reading simultaneously Single-writer automatic retry (handles realistic usage) Works existing permissions model","code":"db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"postgresql-for-production-scale","dir":"","previous_headings":"Choosing a Catalog Backend (DuckLake)","what":"PostgreSQL for Production Scale","title":"Local Data Lake Infrastructure for R","text":"need true multi-user concurrent writes remote access:","code":"db_lake_connect(   catalog_type = \"postgres\",   metadata_path = \"dbname=ducklake_catalog host=db.cso.ie\",   data_path = \"//CSO-NAS/DataLake/data\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"access-control","dir":"","previous_headings":"","what":"Access Control","title":"Local Data Lake Infrastructure for R","text":"modes rely file system permissions access control: Hive mode: Users need read/write access relevant section folders DuckLake mode: Users need access metadata file/database data folder integrates existing infrastructure - new authentication systems needed.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"Local Data Lake Infrastructure for R","text":"vignette(\"concepts\") - Background data lakes, hive partitioning, DuckLake vignette(\"code-walkthrough\") - Detailed explanation package works","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Local Data Lake Infrastructure for R","text":"Found bug feature request? Please open issue GitHub.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser.html","id":null,"dir":"Reference","previous_headings":"","what":"Browse the data lake interactively â€” db_browser","title":"Browse the data lake interactively â€” db_browser","text":"Launches Shiny app browse datasets, view metadata, search data, preview tables.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Browse the data lake interactively â€” db_browser","text":"","code":"db_browser(height = \"500px\", viewer = c(\"dialog\", \"browser\", \"pane\"))"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Browse the data lake interactively â€” db_browser","text":"height Height data preview table (default \"500px\") viewer display: \"dialog\" (RStudio viewer), \"browser\", \"pane\"","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Browse the data lake interactively â€” db_browser","text":"Opens browser app. Returns NULL invisibly.","code":""},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Browse the data lake interactively â€” db_browser","text":"","code":"if (FALSE) { # \\dontrun{ # Connect first db_connect(path = \"//CSO-NAS/DataLake\")  # Launch browser db_browser()  # Or with DuckLake db_lake_connect(...) db_browser() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_server.html","id":null,"dir":"Reference","previous_headings":"","what":"Shiny module server for db_browser() â€” db_browser_server","title":"Shiny module server for db_browser() â€” db_browser_server","text":"Use db_browser_ui() db_browser_server() include data lake browser Shiny module app.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_server.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shiny module server for db_browser() â€” db_browser_server","text":"","code":"db_browser_server(id, height = \"500px\")"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_server.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shiny module server for db_browser() â€” db_browser_server","text":"id Character length 1, module ID (must match UI) height Height data preview tables","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_server.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shiny module server for db_browser() â€” db_browser_server","text":"Shiny module server function","code":""},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_server.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shiny module server for db_browser() â€” db_browser_server","text":"","code":"if (FALSE) { # \\dontrun{ # In your Shiny app: server <- function(input, output, session) {   db_browser_server(\"browser1\") } } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_ui.html","id":null,"dir":"Reference","previous_headings":"","what":"Shiny module UI for db_browser() â€” db_browser_ui","title":"Shiny module UI for db_browser() â€” db_browser_ui","text":"Use db_browser_ui() db_browser_server() include data lake browser Shiny module app.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_ui.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shiny module UI for db_browser() â€” db_browser_ui","text":"","code":"db_browser_ui(id, height = \"500px\")"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_ui.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shiny module UI for db_browser() â€” db_browser_ui","text":"id Character length 1, module ID height Height data preview tables (default \"500px\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_ui.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shiny module UI for db_browser() â€” db_browser_ui","text":"Shiny UI element","code":""},{"path":[]},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_browser_ui.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shiny module UI for db_browser() â€” db_browser_ui","text":"","code":"if (FALSE) { # \\dontrun{ # In your Shiny app UI: ui <- fluidPage(   db_browser_ui(\"browser1\") )  # In your Shiny app server: server <- function(input, output, session) {   db_browser_server(\"browser1\") } } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"List tables and file stats tracked by DuckLake â€” db_catalog","title":"List tables and file stats tracked by DuckLake â€” db_catalog","text":"Returns information tables connected DuckLake catalog, including row counts, file counts, storage statistics.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List tables and file stats tracked by DuckLake â€” db_catalog","text":"","code":"db_catalog()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_catalog.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List tables and file stats tracked by DuckLake â€” db_catalog","text":"data.frame table information","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_catalog.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List tables and file stats tracked by DuckLake â€” db_catalog","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_catalog() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_connect.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to the CSO hive parquet lake â€” db_connect","title":"Connect to the CSO hive parquet lake â€” db_connect","text":"Establishes singleton connection DuckDB stores base path.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_connect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to the CSO hive parquet lake â€” db_connect","text":"","code":"db_connect(   path = \"//CSO-NAS/DataLake\",   db = \":memory:\",   threads = NULL,   memory_limit = NULL,   load_extensions = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_connect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to the CSO hive parquet lake â€” db_connect","text":"path Root path lake (e.g. \"//CSO-NAS/DataLake\") db DuckDB database file path. Use \":memory:\" -memory. threads Number DuckDB threads (NULL leaves default) memory_limit e.g. \"4GB\" (NULL leaves default) load_extensions character vector extensions install/load, e.g. c(\"httpfs\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_connect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to the CSO hive parquet lake â€” db_connect","text":"DuckDB connection object","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_connect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to the CSO hive parquet lake â€” db_connect","text":"","code":"if (FALSE) { # \\dontrun{ # Connect to hive-partitioned data lake db_connect(path = \"//CSO-NAS/DataLake\")  # With performance tuning db_connect(path = \"//CSO-NAS/DataLake\", threads = 4, memory_limit = \"8GB\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_create_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new schema in DuckLake â€” db_create_schema","title":"Create a new schema in DuckLake â€” db_create_schema","text":"Create new schema DuckLake","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_create_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new schema in DuckLake â€” db_create_schema","text":"","code":"db_create_schema(schema)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_create_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new schema in DuckLake â€” db_create_schema","text":"schema Schema name create","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_create_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new schema in DuckLake â€” db_create_schema","text":"Invisibly returns schema name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_create_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a new schema in DuckLake â€” db_create_schema","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_create_schema(\"trade\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_current_section.html","id":null,"dir":"Reference","previous_headings":"","what":"Get current section â€” db_current_section","title":"Get current section â€” db_current_section","text":"Returns name currently connected section (DuckLake mode ).","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_current_section.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get current section â€” db_current_section","text":"","code":"db_current_section()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_current_section.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get current section â€” db_current_section","text":"Section name NULL connected section","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_current_section.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get current section â€” db_current_section","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect_section(\"trade\") db_current_section() #> [1] \"trade\" } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dataset_exists.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a hive dataset exists â€” db_dataset_exists","title":"Check if a hive dataset exists â€” db_dataset_exists","text":"Check hive dataset exists","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dataset_exists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a hive dataset exists â€” db_dataset_exists","text":"","code":"db_dataset_exists(section, dataset)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dataset_exists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a hive dataset exists â€” db_dataset_exists","text":"section section name dataset dataset name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dataset_exists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if a hive dataset exists â€” db_dataset_exists","text":"Logical TRUE exists, FALSE otherwise","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dataset_exists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if a hive dataset exists â€” db_dataset_exists","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() db_dataset_exists(\"Trade\", \"Imports\") # [1] TRUE } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe a dataset or table â€” db_describe","title":"Describe a dataset or table â€” db_describe","text":"Add documentation metadata dataset (hive mode) table (DuckLake mode). Metadata includes description, owner, tags. hive mode, can set public = TRUE publish metadata shared catalog folder, making discoverable organisation-wide without granting access underlying data.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe a dataset or table â€” db_describe","text":"","code":"db_describe(   section = NULL,   dataset = NULL,   schema = \"main\",   table = NULL,   description = NULL,   owner = NULL,   tags = NULL,   public = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe a dataset or table â€” db_describe","text":"section Section name (hive mode ) dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode ) description Free-text description dataset/table owner Owner name team responsible data tags Character vector tags categorization public Logical. TRUE, publish metadata shared catalog folder. FALSE, remove catalog. NULL (default), keep current public status auto-sync already public. (Hive mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Describe a dataset or table â€” db_describe","text":"Invisibly returns metadata list","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe a dataset or table â€” db_describe","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect() db_describe(   section = \"Trade\",   dataset = \"Imports\",   description = \"Monthly import values by country and commodity code\",   owner = \"Trade Section\",   tags = c(\"trade\", \"monthly\", \"official\"),   public = TRUE )  # DuckLake mode db_lake_connect() db_describe(   table = \"imports\",   description = \"Monthly import values\",   owner = \"Trade Section\" ) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe a column â€” db_describe_column","title":"Describe a column â€” db_describe_column","text":"Add documentation specific column dataset table. hive mode, can set public = TRUE include column documentation public catalog. dataset must already public (use db_describe(public = TRUE) first).","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe a column â€” db_describe_column","text":"","code":"db_describe_column(   section = NULL,   dataset = NULL,   schema = \"main\",   table = NULL,   column,   description = NULL,   units = NULL,   notes = NULL,   public = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe a column â€” db_describe_column","text":"section Section name (hive mode ) dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode ) column Column name document description Description column contains units Units measurement (optional) notes Additional notes (optional) public Logical. TRUE, sync column docs public catalog (requires dataset already public). NULL (default), auto-sync dataset already public. (Hive mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Describe a column â€” db_describe_column","text":"Invisibly returns column metadata","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_describe_column.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe a column â€” db_describe_column","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() db_describe_column(   section = \"Trade\",   dataset = \"Imports\",   column = \"value\",   description = \"Import value in thousands\",   units = \"EUR (thousands)\",   public = TRUE ) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dictionary.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a data dictionary â€” db_dictionary","title":"Generate a data dictionary â€” db_dictionary","text":"Creates data dictionary summarizing datasets/tables documentation, schemas, column information.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dictionary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a data dictionary â€” db_dictionary","text":"","code":"db_dictionary(section = NULL, schema = NULL, include_columns = TRUE)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dictionary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a data dictionary â€” db_dictionary","text":"section Limit specific section (hive mode, optional) schema Limit specific schema (DuckLake mode, optional) include_columns Include column-level details (default TRUE)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dictionary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a data dictionary â€” db_dictionary","text":"data.frame data dictionary","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_dictionary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a data dictionary â€” db_dictionary","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() dict <- db_dictionary()  # Export to Excel writexl::write_xlsx(dict, \"data_dictionary.xlsx\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_diff.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare a table between two snapshots â€” db_diff","title":"Compare a table between two snapshots â€” db_diff","text":"Shows differences table two snapshot versions timestamps. Returns added, removed, (optionally) changed rows.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_diff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare a table between two snapshots â€” db_diff","text":"","code":"db_diff(   schema = \"main\",   table,   from_version = NULL,   to_version = NULL,   from_timestamp = NULL,   to_timestamp = NULL,   key_cols = NULL,   collect = TRUE )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_diff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare a table between two snapshots â€” db_diff","text":"schema Schema name (default \"main\") table Table name from_version Starting snapshot version (integer) NULL use from_timestamp to_version Ending snapshot version (integer, default: current) NULL use to_timestamp from_timestamp Starting timestamp (alternative from_version) to_timestamp Ending timestamp (alternative to_version, default: current) key_cols Character vector columns uniquely identify rows. provided, enables detection modified rows (just added/removed). collect TRUE (default), returns collected data.frames. FALSE, returns lazy tbl references.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_diff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare a table between two snapshots â€” db_diff","text":"list components: added, removed, (key_cols provided) modified","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_diff.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare a table between two snapshots â€” db_diff","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  # Compare versions 3 and 5 diff <- db_diff(table = \"products\", from_version = 3, to_version = 5) diff$added diff$removed  # Compare with key columns to see modifications diff <- db_diff(table = \"products\", from_version = 3, to_version = 5,                 key_cols = \"product_id\") diff$modified } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_disconnect.html","id":null,"dir":"Reference","previous_headings":"","what":"Disconnect from the CSO Data Lake â€” db_disconnect","title":"Disconnect from the CSO Data Lake â€” db_disconnect","text":"Disconnect CSO Data Lake","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_disconnect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disconnect from the CSO Data Lake â€” db_disconnect","text":"","code":"db_disconnect()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_disconnect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Disconnect from the CSO Data Lake â€” db_disconnect","text":"Invisibly returns TRUE disconnected, FALSE connected.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_disconnect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Disconnect from the CSO Data Lake â€” db_disconnect","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() # ... do work ... db_disconnect() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_get_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get documentation for a dataset or table â€” db_get_docs","title":"Get documentation for a dataset or table â€” db_get_docs","text":"Retrieve documentation metadata dataset table.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_get_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get documentation for a dataset or table â€” db_get_docs","text":"","code":"db_get_docs(section = NULL, dataset = NULL, schema = \"main\", table = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_get_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get documentation for a dataset or table â€” db_get_docs","text":"section Section name (hive mode ) dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_get_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get documentation for a dataset or table â€” db_get_docs","text":"list containing description, owner, tags, column documentation","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_get_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get documentation for a dataset or table â€” db_get_docs","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() db_get_docs(\"Trade\", \"Imports\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","title":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","text":"Read CSO Dataset hive-partitioned parquet (lazy)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","text":"","code":"db_hive_read(section, dataset, ...)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","text":"section section name (e.g. \"Trade\") dataset dataset name (e.g. \"Imports\") ... Additional named options passed DuckDB read_parquet() SQL form (e.g. union_by_name = TRUE). See examples .","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","text":"lazy tbl_duckdb object","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a CSO Dataset from hive-partitioned parquet (lazy) â€” db_hive_read","text":"","code":"if (FALSE) { # \\dontrun{ # Basic read db_hive_read(\"Trade\", \"Imports\")  # With options db_hive_read(\"Trade\", \"Imports\", union_by_name = TRUE, filename = TRUE) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","title":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","text":"Publish / Append / Ignore / Replace Partitions Hive Lake","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","text":"","code":"db_hive_write(   data,   section,   dataset,   partition_by = NULL,   mode = c(\"overwrite\", \"append\", \"ignore\", \"replace_partitions\"),   compression = NULL,   filename_pattern = \"data_{uuid}\" )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","text":"data data.frame / tibble section section name dataset name dataset partition_by Character vector column names partition (e.g. c(\"year\",\"month\")) mode One : \"overwrite\": replace target files \"append\": add new files (requires unique filenames) \"ignore\": write target path exist (best-effort; still race-prone) \"replace_partitions\": delete affected partition folders, append fresh files (requires partition_by) compression Parquet compression codec (NULL means DuckDB default). Options: \"zstd\", \"snappy\", \"gzip\", \"brotli\", \"lz4\", \"lz4_raw\", \"uncompressed\" filename_pattern Used append-like modes (default \"data_uuid\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","text":"Invisibly returns output path","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_hive_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Publish / Append / Ignore / Replace Partitions in the Hive Lake â€” db_hive_write","text":"","code":"if (FALSE) { # \\dontrun{ # Basic overwrite db_hive_write(my_data, \"Trade\", \"Imports\")  # Partitioned write db_hive_write(my_data, \"Trade\", \"Imports\", partition_by = c(\"year\", \"month\"))  # Append mode db_hive_write(my_data, \"Trade\", \"Imports\", mode = \"append\")  # Replace only touched partitions db_hive_write(my_data, \"Trade\", \"Imports\",                partition_by = c(\"year\", \"month\"),                mode = \"replace_partitions\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_is_public.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a dataset/table is in the public catalog â€” db_is_public","title":"Check if a dataset/table is in the public catalog â€” db_is_public","text":"Check whether metadata published discovery catalog.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_is_public.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a dataset/table is in the public catalog â€” db_is_public","text":"","code":"db_is_public(section = NULL, dataset = NULL, schema = \"main\", table = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_is_public.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a dataset/table is in the public catalog â€” db_is_public","text":"section Section name (hive mode), NULL DuckLake mode dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_is_public.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if a dataset/table is in the public catalog â€” db_is_public","text":"Logical TRUE public, FALSE otherwise","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_is_public.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if a dataset/table is in the public catalog â€” db_is_public","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect(\"//CSO-NAS/DataLake\") db_is_public(section = \"Trade\", dataset = \"Imports\")  # DuckLake mode db_lake_connect_section(\"trade\") db_is_public(schema = \"main\", table = \"imports\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","title":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","text":"Connect DuckDB + attach DuckLake catalog","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","text":"","code":"db_lake_connect(   duckdb_db = \":memory:\",   catalog = \"cso\",   catalog_type = c(\"duckdb\", \"sqlite\", \"postgres\"),   metadata_path = \"metadata.ducklake\",   data_path = \"//CSO-NAS/DataLake\",   snapshot_version = NULL,   snapshot_time = NULL,   threads = NULL,   memory_limit = NULL,   load_extensions = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","text":"duckdb_db DuckDB database file path. Use \":memory:\" -memory. catalog DuckLake catalog name inside DuckDB (e.g. \"cso\") catalog_type Type catalog database backend. One : \"duckdb\" (default): Single-client local use. Metadata stored .ducklake file. \"sqlite\": Multi-client local use. Metadata stored .sqlite file. Supports multiple readers + single writer automatic retry. Recommended CSO use cases shared network drives. \"postgres\": Multi-user lakehouse. Metadata stored PostgreSQL database. Requires PostgreSQL 12+ connection string metadata_path. metadata_path Path connection string DuckLake metadata: \"duckdb\": file path (e.g. \"metadata.ducklake\") \"sqlite\": file path (e.g. \"//CSO-NAS/DataLake/catalog.sqlite\") \"postgres\": connection string (e.g. \"dbname=ducklake_catalog host=localhost\") data_path Root storage path DuckLake writes Parquet data files snapshot_version Optional integer snapshot version attach snapshot_time Optional timestamp string attach (e.g. \"2025-05-26 00:00:00\") threads Number DuckDB threads (NULL leaves default) memory_limit e.g. \"4GB\" (NULL leaves default) load_extensions character vector extensions install/load, e.g. c(\"httpfs\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","text":"DuckDB connection object","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to DuckDB + attach a DuckLake catalog â€” db_lake_connect","text":"","code":"if (FALSE) { # \\dontrun{ # DuckDB catalog (single user, simplest setup) db_lake_connect(   metadata_path = \"metadata.ducklake\",   data_path = \"//CSO-NAS/DataLake\" )  # SQLite catalog (multiple local users - RECOMMENDED for shared drives) db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"//CSO-NAS/DataLake/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\" )  # PostgreSQL catalog (multi-user lakehouse, remote clients) db_lake_connect(   catalog_type = \"postgres\",   metadata_path = \"dbname=ducklake_catalog host=db.cso.ie user=analyst\",   data_path = \"//CSO-NAS/DataLake/data\" )  # Time travel - connect to a specific snapshot db_lake_connect(   catalog_type = \"sqlite\",   metadata_path = \"catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/data\",   snapshot_version = 5 ) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect_section.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","title":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","text":"Connects registered section looking catalog data paths master discovery catalog. enables public parameter db_describe() sync master catalog.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect_section.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","text":"","code":"db_lake_connect_section(   section,   master_path = NULL,   duckdb_db = \":memory:\",   catalog = NULL,   catalog_type = NULL,   threads = NULL,   memory_limit = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect_section.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","text":"section Section name (must registered master catalog) master_path Path master catalog (uses default specified) duckdb_db DuckDB database file path. Use \":memory:\" -memory. catalog DuckLake catalog name inside DuckDB catalog_type Type catalog backend (auto-detected path specified) threads Number DuckDB threads (NULL leaves default) memory_limit e.g. \"4GB\" (NULL leaves default)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect_section.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","text":"DuckDB connection object","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_connect_section.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to a DuckLake section via master catalog â€” db_lake_connect_section","text":"","code":"if (FALSE) { # \\dontrun{ # Connect to a section registered in master catalog db_lake_connect_section(\"trade\")  # Now db_describe(public=TRUE) will sync to master catalog db_describe(table = \"imports\", description = \"...\", public = TRUE) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a DuckLake table (lazy) â€” db_lake_read","title":"Read a DuckLake table (lazy) â€” db_lake_read","text":"Read DuckLake table (lazy)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a DuckLake table (lazy) â€” db_lake_read","text":"","code":"db_lake_read(schema = \"main\", table, version = NULL, timestamp = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a DuckLake table (lazy) â€” db_lake_read","text":"schema Schema name (default \"main\") table Table name version Optional integer snapshot version time travel timestamp Optional timestamp string time travel (e.g. \"2025-05-26 00:00:00\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a DuckLake table (lazy) â€” db_lake_read","text":"lazy tbl_duckdb object","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a DuckLake table (lazy) â€” db_lake_read","text":"","code":"if (FALSE) { # \\dontrun{ # Basic read db_lake_read(table = \"imports\")  # From a specific schema db_lake_read(schema = \"trade\", table = \"imports\")  # Time travel by version db_lake_read(table = \"imports\", version = 5)  # Time travel by timestamp db_lake_read(table = \"imports\", timestamp = \"2025-05-26 00:00:00\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a DuckLake table (overwrite/append) â€” db_lake_write","title":"Write a DuckLake table (overwrite/append) â€” db_lake_write","text":"Write DuckLake table (overwrite/append)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a DuckLake table (overwrite/append) â€” db_lake_write","text":"","code":"db_lake_write(   data,   schema = \"main\",   table,   mode = c(\"overwrite\", \"append\"),   commit_author = NULL,   commit_message = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a DuckLake table (overwrite/append) â€” db_lake_write","text":"data data.frame/tibble schema Schema name (default \"main\") table Table name mode \"overwrite\" \"append\" commit_author Optional author DuckLake commit metadata commit_message Optional message DuckLake commit metadata","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a DuckLake table (overwrite/append) â€” db_lake_write","text":"Invisibly returns qualified table name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_lake_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write a DuckLake table (overwrite/append) â€” db_lake_write","text":"","code":"if (FALSE) { # \\dontrun{ # Basic overwrite db_lake_write(my_data, table = \"imports\")  # With schema db_lake_write(my_data, schema = \"trade\", table = \"imports\")  # Append mode with commit info db_lake_write(my_data, table = \"imports\", mode = \"append\",               commit_author = \"jsmith\",                commit_message = \"Added Q3 data\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"List datasets within a section â€” db_list_datasets","title":"List datasets within a section â€” db_list_datasets","text":"Returns datasets (subfolders) within given section.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List datasets within a section â€” db_list_datasets","text":"","code":"db_list_datasets(section)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_datasets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List datasets within a section â€” db_list_datasets","text":"section section name (e.g. \"Trade\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_datasets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List datasets within a section â€” db_list_datasets","text":"Character vector dataset names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_datasets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List datasets within a section â€” db_list_datasets","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() db_list_datasets(\"Trade\") # [1] \"Imports\" \"Exports\" \"Balance\" } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_public.html","id":null,"dir":"Reference","previous_headings":"","what":"List all datasets/tables in the public catalog â€” db_list_public","title":"List all datasets/tables in the public catalog â€” db_list_public","text":"Lists entries published discovery catalog. works even access underlying data, allowing organisation-wide data discovery.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_public.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all datasets/tables in the public catalog â€” db_list_public","text":"","code":"db_list_public(section = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_public.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all datasets/tables in the public catalog â€” db_list_public","text":"section Optional section filter ","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_public.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all datasets/tables in the public catalog â€” db_list_public","text":"data.frame discovery information","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_public.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List all datasets/tables in the public catalog â€” db_list_public","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect(\"//CSO-NAS/DataLake\") db_list_public() db_list_public(section = \"Trade\")  # DuckLake mode - lists from master catalog db_lake_connect_section(\"trade\") db_list_public() db_list_public(section = \"trade\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_registered_sections.html","id":null,"dir":"Reference","previous_headings":"","what":"List registered sections â€” db_list_registered_sections","title":"List registered sections â€” db_list_registered_sections","text":"Lists sections registered master discovery catalog.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_registered_sections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List registered sections â€” db_list_registered_sections","text":"","code":"db_list_registered_sections(master_path = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_registered_sections.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List registered sections â€” db_list_registered_sections","text":"master_path Path master catalog (uses default specified)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_registered_sections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List registered sections â€” db_list_registered_sections","text":"data.frame section information","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_registered_sections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List registered sections â€” db_list_registered_sections","text":"","code":"if (FALSE) { # \\dontrun{ db_list_registered_sections() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_schemas.html","id":null,"dir":"Reference","previous_headings":"","what":"List schemas in the DuckLake catalog â€” db_list_schemas","title":"List schemas in the DuckLake catalog â€” db_list_schemas","text":"Returns schemas connected DuckLake catalog.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_schemas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List schemas in the DuckLake catalog â€” db_list_schemas","text":"","code":"db_list_schemas()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_schemas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List schemas in the DuckLake catalog â€” db_list_schemas","text":"Character vector schema names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_schemas.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List schemas in the DuckLake catalog â€” db_list_schemas","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_list_schemas() # [1] \"main\" \"trade\" \"labour\" } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_sections.html","id":null,"dir":"Reference","previous_headings":"","what":"List sections in the hive data lake â€” db_list_sections","title":"List sections in the hive data lake â€” db_list_sections","text":"Returns top-level section folders data lake.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_sections.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List sections in the hive data lake â€” db_list_sections","text":"","code":"db_list_sections()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_sections.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List sections in the hive data lake â€” db_list_sections","text":"Character vector section names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_sections.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List sections in the hive data lake â€” db_list_sections","text":"","code":"if (FALSE) { # \\dontrun{ db_connect() db_list_sections() # [1] \"Trade\" \"Labour\" \"Health\" \"Agriculture\" } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"List tables in a DuckLake schema â€” db_list_tables","title":"List tables in a DuckLake schema â€” db_list_tables","text":"Returns tables given schema.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List tables in a DuckLake schema â€” db_list_tables","text":"","code":"db_list_tables(schema = \"main\")"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List tables in a DuckLake schema â€” db_list_tables","text":"schema Schema name (default \"main\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_tables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List tables in a DuckLake schema â€” db_list_tables","text":"Character vector table names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_tables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List tables in a DuckLake schema â€” db_list_tables","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_list_tables() # [1] \"imports\" \"exports\" \"products\"  db_list_tables(\"trade\") # [1] \"monthly_summary\" \"annual_totals\" } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_views.html","id":null,"dir":"Reference","previous_headings":"","what":"List views in a DuckLake schema â€” db_list_views","title":"List views in a DuckLake schema â€” db_list_views","text":"Returns views given schema.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_views.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List views in a DuckLake schema â€” db_list_views","text":"","code":"db_list_views(schema = \"main\")"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_views.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List views in a DuckLake schema â€” db_list_views","text":"schema Schema name (default \"main\")","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_list_views.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List views in a DuckLake schema â€” db_list_views","text":"Character vector view names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_hive_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Preview a hive write operation â€” db_preview_hive_write","title":"Preview a hive write operation â€” db_preview_hive_write","text":"Shows happen ran db_hive_write() without actually writing data. Useful validating writes execution.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_hive_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preview a hive write operation â€” db_preview_hive_write","text":"","code":"db_preview_hive_write(   data,   section,   dataset,   partition_by = NULL,   mode = c(\"overwrite\", \"append\", \"ignore\", \"replace_partitions\") )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_hive_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preview a hive write operation â€” db_preview_hive_write","text":"data data.frame / tibble section section name dataset name dataset partition_by Character vector column names partition (e.g. c(\"year\",\"month\")) mode One : \"overwrite\": replace target files \"append\": add new files (requires unique filenames) \"ignore\": write target path exist (best-effort; still race-prone) \"replace_partitions\": delete affected partition folders, append fresh files (requires partition_by)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_hive_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preview a hive write operation â€” db_preview_hive_write","text":"list preview information (invisibly), also prints summary","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_hive_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preview a hive write operation â€” db_preview_hive_write","text":"","code":"if (FALSE) { # \\dontrun{ db_connect()  # Preview before writing db_preview_hive_write(my_data, \"Trade\", \"Imports\", partition_by = \"year\")  # If preview looks good, actually write db_hive_write(my_data, \"Trade\", \"Imports\", partition_by = \"year\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_lake_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Preview a DuckLake write operation â€” db_preview_lake_write","title":"Preview a DuckLake write operation â€” db_preview_lake_write","text":"Shows happen ran db_lake_write() without actually writing data.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_lake_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preview a DuckLake write operation â€” db_preview_lake_write","text":"","code":"db_preview_lake_write(   data,   schema = \"main\",   table,   mode = c(\"overwrite\", \"append\") )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_lake_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preview a DuckLake write operation â€” db_preview_lake_write","text":"data data.frame/tibble schema Schema name (default \"main\") table Table name mode \"overwrite\" \"append\"","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_lake_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preview a DuckLake write operation â€” db_preview_lake_write","text":"list preview information (invisibly), also prints summary","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_lake_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preview a DuckLake write operation â€” db_preview_lake_write","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  db_preview_lake_write(my_data, table = \"products\", mode = \"overwrite\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_upsert.html","id":null,"dir":"Reference","previous_headings":"","what":"Preview an upsert operation â€” db_preview_upsert","title":"Preview an upsert operation â€” db_preview_upsert","text":"Shows happen ran db_upsert() - many rows inserted vs updated.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_upsert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preview an upsert operation â€” db_preview_upsert","text":"","code":"db_preview_upsert(data, schema = \"main\", table, by, update_cols = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_upsert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preview an upsert operation â€” db_preview_upsert","text":"data data.frame / tibble schema Schema name (default \"main\") table Table name Character vector key columns used match rows update_cols Controls columns update match: NULL (default): update columns character(0): insert-(updates match) character vector: update specified columns","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_upsert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preview an upsert operation â€” db_preview_upsert","text":"list preview information (invisibly), also prints summary","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_preview_upsert.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preview an upsert operation â€” db_preview_upsert","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  db_preview_upsert(my_data, table = \"products\", by = \"product_id\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Run arbitrary SQL and return results â€” db_query","title":"Run arbitrary SQL and return results â€” db_query","text":"Escape hatch power users need run custom SQL queries.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run arbitrary SQL and return results â€” db_query","text":"","code":"db_query(sql, collect = TRUE)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run arbitrary SQL and return results â€” db_query","text":"sql SQL query string collect TRUE (default), returns collected data.frame. FALSE, returns lazy tbl reference.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run arbitrary SQL and return results â€” db_query","text":"Query results data.frame lazy tbl","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run arbitrary SQL and return results â€” db_query","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  # Run a custom query db_query(\"SELECT * FROM main.products WHERE price > 100\")  # Get a lazy reference lazy_result <- db_query(\"SELECT * FROM main.products\", collect = FALSE) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_register_section.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a section in the master catalog â€” db_register_section","title":"Register a section in the master catalog â€” db_register_section","text":"Registers DuckLake section (catalog) master discovery catalog, making discoverable organisation-wide.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_register_section.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a section in the master catalog â€” db_register_section","text":"","code":"db_register_section(   section,   catalog_path,   data_path,   description = NULL,   owner = NULL,   master_path = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_register_section.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a section in the master catalog â€” db_register_section","text":"section Section name (e.g., \"trade\", \"labour\") catalog_path Path section's DuckLake catalog file data_path Path section's data folder description Optional description section owner Optional owner/team name master_path Path master catalog (uses default specified)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_register_section.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register a section in the master catalog â€” db_register_section","text":"Invisibly returns TRUE","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_register_section.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a section in the master catalog â€” db_register_section","text":"","code":"if (FALSE) { # \\dontrun{ db_register_section(   section = \"trade\",   catalog_path = \"//CSO-NAS/DataLake/trade/catalog.sqlite\",   data_path = \"//CSO-NAS/DataLake/trade/data\",   owner = \"Trade Team\" ) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_rollback.html","id":null,"dir":"Reference","previous_headings":"","what":"Rollback a table to a previous snapshot â€” db_rollback","title":"Rollback a table to a previous snapshot â€” db_rollback","text":"Restores table state specific snapshot version timestamp. creates new snapshot rolled-back data.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_rollback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rollback a table to a previous snapshot â€” db_rollback","text":"","code":"db_rollback(   schema = \"main\",   table,   version = NULL,   timestamp = NULL,   commit_author = NULL,   commit_message = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_rollback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rollback a table to a previous snapshot â€” db_rollback","text":"schema Schema name (default \"main\") table Table name version Snapshot version rollback (integer) timestamp Timestamp rollback (POSIXct character string) commit_author Optional author rollback commit commit_message Optional message rollback commit (defaults auto-generated)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_rollback.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rollback a table to a previous snapshot â€” db_rollback","text":"Invisibly returns qualified table name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_rollback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rollback a table to a previous snapshot â€” db_rollback","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  # Rollback to a specific version db_rollback(table = \"products\", version = 5)  # Rollback to a specific time db_rollback(table = \"products\", timestamp = \"2025-01-15 00:00:00\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for datasets or tables â€” db_search","title":"Search for datasets or tables â€” db_search","text":"Search datasets/tables name, description, owner, tags.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for datasets or tables â€” db_search","text":"","code":"db_search(pattern, field = c(\"all\", \"name\", \"description\", \"owner\", \"tags\"))"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for datasets or tables â€” db_search","text":"pattern Search pattern (case-insensitive, matches partial strings) field Field search: \"\" (default), \"name\", \"description\", \"owner\", \"tags\"","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for datasets or tables â€” db_search","text":"data.frame matching datasets/tables documentation","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for datasets or tables â€” db_search","text":"","code":"if (FALSE) { # \\dontrun{ db_connect()  # Search everywhere db_search(\"trade\")  # Search only tags db_search(\"official\", field = \"tags\")  # Search by owner db_search(\"Trade Section\", field = \"owner\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search_columns.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for columns â€” db_search_columns","title":"Search for columns â€” db_search_columns","text":"Search columns name across datasets/tables.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search_columns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for columns â€” db_search_columns","text":"","code":"db_search_columns(pattern)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search_columns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for columns â€” db_search_columns","text":"pattern Column name pattern (case-insensitive, matches partial strings)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search_columns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for columns â€” db_search_columns","text":"data.frame matching columns table/dataset info","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_search_columns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for columns â€” db_search_columns","text":"","code":"if (FALSE) { # \\dontrun{ db_connect()  # Find all columns containing \"country\" db_search_columns(\"country\")  # Find all ID columns db_search_columns(\"_id\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_private.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a dataset/table from the public catalog â€” db_set_private","title":"Remove a dataset/table from the public catalog â€” db_set_private","text":"Removes metadata public discovery catalog. dataset/table data remain unchanged.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_private.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a dataset/table from the public catalog â€” db_set_private","text":"","code":"db_set_private(section = NULL, dataset = NULL, schema = \"main\", table = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_private.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a dataset/table from the public catalog â€” db_set_private","text":"section Section name (hive mode), NULL DuckLake mode dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_private.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a dataset/table from the public catalog â€” db_set_private","text":"Invisibly returns TRUE","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_private.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a dataset/table from the public catalog â€” db_set_private","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect(\"//CSO-NAS/DataLake\") db_set_private(section = \"Trade\", dataset = \"Imports\")  # DuckLake mode db_lake_connect_section(\"trade\") db_set_private(schema = \"main\", table = \"imports\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_public.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a dataset/table discoverable in the public catalog â€” db_set_public","title":"Make a dataset/table discoverable in the public catalog â€” db_set_public","text":"Makes metadata discoverable organisation-wide. hive mode: Copies metadata shared _catalog/ folder. DuckLake mode: Publishes master discovery catalog (requires section).","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_public.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a dataset/table discoverable in the public catalog â€” db_set_public","text":"","code":"db_set_public(section = NULL, dataset = NULL, schema = \"main\", table = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_public.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a dataset/table discoverable in the public catalog â€” db_set_public","text":"section Section name (hive mode), NULL DuckLake mode dataset Dataset name (hive mode ) schema Schema name (DuckLake mode, default \"main\") table Table name (DuckLake mode )","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_public.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a dataset/table discoverable in the public catalog â€” db_set_public","text":"Invisibly returns TRUE","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_set_public.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a dataset/table discoverable in the public catalog â€” db_set_public","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect(\"//CSO-NAS/DataLake\") db_set_public(section = \"Trade\", dataset = \"Imports\")  # DuckLake mode db_lake_connect_section(\"trade\") db_set_public(schema = \"main\", table = \"imports\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_setup_master.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up the master discovery catalog â€” db_setup_master","title":"Set up the master discovery catalog â€” db_setup_master","text":"Creates master discovery catalog SQLite database required schema. one-time admin task.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_setup_master.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up the master discovery catalog â€” db_setup_master","text":"","code":"db_setup_master(master_path)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_setup_master.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up the master discovery catalog â€” db_setup_master","text":"master_path Path master catalog SQLite file","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_setup_master.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up the master discovery catalog â€” db_setup_master","text":"Invisibly returns master_path","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_setup_master.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set up the master discovery catalog â€” db_setup_master","text":"","code":"if (FALSE) { # \\dontrun{ db_setup_master(\"//CSO-NAS/DataLake/_master/discovery.sqlite\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_snapshots.html","id":null,"dir":"Reference","previous_headings":"","what":"List DuckLake snapshots â€” db_snapshots","title":"List DuckLake snapshots â€” db_snapshots","text":"Returns snapshots (versions) connected DuckLake catalog, including snapshot ID, timestamp, commit metadata.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_snapshots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List DuckLake snapshots â€” db_snapshots","text":"","code":"db_snapshots()"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_snapshots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List DuckLake snapshots â€” db_snapshots","text":"data.frame snapshot information","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_snapshots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List DuckLake snapshots â€” db_snapshots","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_snapshots() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get connection status and configuration â€” db_status","title":"Get connection status and configuration â€” db_status","text":"Returns information current connection state, including mode (hive/ducklake), paths, connection validity.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get connection status and configuration â€” db_status","text":"","code":"db_status(verbose = TRUE)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get connection status and configuration â€” db_status","text":"verbose TRUE, prints formatted summary. FALSE, returns list silently.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_status.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get connection status and configuration â€” db_status","text":"list (invisibly verbose=TRUE) containing connection details.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_status.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get connection status and configuration â€” db_status","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_status() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_switch_section.html","id":null,"dir":"Reference","previous_headings":"","what":"Switch to a different section â€” db_switch_section","title":"Switch to a different section â€” db_switch_section","text":"Disconnects current section connects different one. Requires master catalog configured.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_switch_section.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Switch to a different section â€” db_switch_section","text":"","code":"db_switch_section(section)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_switch_section.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Switch to a different section â€” db_switch_section","text":"section Section name switch ","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_switch_section.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Switch to a different section â€” db_switch_section","text":"DuckDB connection object (invisibly)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_switch_section.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Switch to a different section â€” db_switch_section","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect_section(\"trade\") db_switch_section(\"labour\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_sync_catalog.html","id":null,"dir":"Reference","previous_headings":"","what":"Sync the public catalog with source metadata â€” db_sync_catalog","title":"Sync the public catalog with source metadata â€” db_sync_catalog","text":"Scans public catalog updates entries source metadata. Optionally removes entries source longer exists.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_sync_catalog.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sync the public catalog with source metadata â€” db_sync_catalog","text":"","code":"db_sync_catalog(remove_orphans = FALSE)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_sync_catalog.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sync the public catalog with source metadata â€” db_sync_catalog","text":"remove_orphans Logical. TRUE, remove catalog entries source longer exists. Default FALSE.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_sync_catalog.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sync the public catalog with source metadata â€” db_sync_catalog","text":"Invisibly returns list counts synced, removed, errors","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_sync_catalog.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sync the public catalog with source metadata â€” db_sync_catalog","text":"","code":"if (FALSE) { # \\dontrun{ # Hive mode db_connect(\"//CSO-NAS/DataLake\") db_sync_catalog() db_sync_catalog(remove_orphans = TRUE)  # DuckLake mode db_lake_connect_section(\"trade\") db_sync_catalog() } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Get column names for a DuckLake table â€” db_table_cols","title":"Get column names for a DuckLake table â€” db_table_cols","text":"Get column names DuckLake table","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get column names for a DuckLake table â€” db_table_cols","text":"","code":"db_table_cols(schema = \"main\", table)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get column names for a DuckLake table â€” db_table_cols","text":"schema Schema name (default \"main\") table Table name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get column names for a DuckLake table â€” db_table_cols","text":"Character vector column names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_exists.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a DuckLake table exists â€” db_table_exists","title":"Check if a DuckLake table exists â€” db_table_exists","text":"Check DuckLake table exists","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_exists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a DuckLake table exists â€” db_table_exists","text":"","code":"db_table_exists(schema = \"main\", table)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_exists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a DuckLake table exists â€” db_table_exists","text":"schema Schema name (default \"main\") table Table name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_exists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if a DuckLake table exists â€” db_table_exists","text":"Logical TRUE exists, FALSE otherwise","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_table_exists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if a DuckLake table exists â€” db_table_exists","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect() db_table_exists(table = \"imports\") # [1] TRUE } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_unregister_section.html","id":null,"dir":"Reference","previous_headings":"","what":"Unregister a section from the master catalog â€” db_unregister_section","title":"Unregister a section from the master catalog â€” db_unregister_section","text":"Removes section master discovery catalog. delete section's data catalog.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_unregister_section.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unregister a section from the master catalog â€” db_unregister_section","text":"","code":"db_unregister_section(section, master_path = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_unregister_section.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unregister a section from the master catalog â€” db_unregister_section","text":"section Section name master_path Path master catalog (uses default specified)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_unregister_section.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unregister a section from the master catalog â€” db_unregister_section","text":"Invisibly returns TRUE","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_unregister_section.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unregister a section from the master catalog â€” db_unregister_section","text":"","code":"if (FALSE) { # \\dontrun{ db_unregister_section(\"trade\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_upsert.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","title":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","text":"Upsert DuckLake table using MERGE ","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_upsert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","text":"","code":"db_upsert(   data,   schema = \"main\",   table,   by,   strict = TRUE,   update_cols = NULL,   commit_author = NULL,   commit_message = NULL )"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_upsert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","text":"data data.frame / tibble schema Schema name (default \"main\") table Table name Character vector key columns used match rows strict TRUE (default), refuse upsert duplicates exist data key. update_cols Controls columns update match: NULL (default): update columns character(0): insert-(updates match) character vector: update specified columns commit_author Optional DuckLake commit author commit_message Optional DuckLake commit message","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_upsert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","text":"Invisibly returns qualified table name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_upsert.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upsert into a DuckLake table using MERGE INTO â€” db_upsert","text":"","code":"if (FALSE) { # \\dontrun{ # Basic upsert by id db_upsert(my_data, table = \"products\", by = \"product_id\")  # Composite key db_upsert(my_data, table = \"sales\", by = c(\"region\", \"date\"))  # Update only specific columns db_upsert(my_data, table = \"products\", by = \"product_id\",           update_cols = c(\"price\", \"updated_at\"))  # Insert-only (no updates) db_upsert(my_data, table = \"events\", by = \"event_id\",           update_cols = character(0))  # With commit metadata db_upsert(my_data, table = \"products\", by = \"product_id\",           commit_author = \"jsmith\",           commit_message = \"Price update batch\") } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_vacuum.html","id":null,"dir":"Reference","previous_headings":"","what":"Vacuum old snapshots from DuckLake â€” db_vacuum","title":"Vacuum old snapshots from DuckLake â€” db_vacuum","text":"Removes old snapshots associated data files longer needed. reclaims storage space deleting data referenced snapshot within retention period.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_vacuum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vacuum old snapshots from DuckLake â€” db_vacuum","text":"","code":"db_vacuum(older_than = \"30 days\", dry_run = TRUE)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_vacuum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vacuum old snapshots from DuckLake â€” db_vacuum","text":"older_than Snapshots older removed. Can : difftime lubridate duration (e.g. .difftime(7, units = \"days\")) character string parseable DuckDB (e.g. \"7 days\", \"1 month\") POSIXct timestamp (snapshots time removed) dry_run TRUE (default), reports deleted without actually deleting. Set FALSE perform actual cleanup.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_vacuum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vacuum old snapshots from DuckLake â€” db_vacuum","text":"data.frame summarising () cleaned ","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_vacuum.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Vacuum old snapshots from DuckLake â€” db_vacuum","text":"","code":"if (FALSE) { # \\dontrun{ db_lake_connect()  # See what would be cleaned up (dry run) db_vacuum(older_than = \"30 days\")  # Actually clean up db_vacuum(older_than = \"30 days\", dry_run = FALSE) } # }"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_view_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Get column names for a DuckLake view â€” db_view_cols","title":"Get column names for a DuckLake view â€” db_view_cols","text":"Get column names DuckLake view","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_view_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get column names for a DuckLake view â€” db_view_cols","text":"","code":"db_view_cols(schema = \"main\", view)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_view_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get column names for a DuckLake view â€” db_view_cols","text":"schema Schema name (default \"main\") view View name","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/db_view_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get column names for a DuckLake view â€” db_view_cols","text":"Character vector column names","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/run_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a package example script â€” run_example","title":"Run a package example script â€” run_example","text":"Runs one example scripts included package.","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/run_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a package example script â€” run_example","text":"","code":"run_example(name = NULL)"},{"path":"https://cathalbyrnegit.github.io/datapond/reference/run_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a package example script â€” run_example","text":"name Name example (without .R extension)","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/run_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a package example script â€” run_example","text":"Runs example script","code":""},{"path":"https://cathalbyrnegit.github.io/datapond/reference/run_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a package example script â€” run_example","text":"","code":"if (FALSE) { # \\dontrun{ # List available examples run_example()  # Run specific example run_example(\"browser_demo_hive\") run_example(\"browser_demo_ducklake\") } # }"}]
